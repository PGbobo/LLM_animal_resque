import pymysql.cursors
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import csv
import time
import schedule
from datetime import datetime
from datetime import date 
import re
import sys
import math

# ====================================================================
# 1. í™˜ê²½ ì„¤ì • 
# ====================================================================
DB_CONFIG = {
    "host": "project-db-campus.smhrd.com",
    "port": 3307, 
    "user": "campus_24IS_CLOUD3_p3_1",
    "password": "smhrd1", 
    "database": "campus_24IS_CLOUD3_p3_1",
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

CRAWL_URL = "https://www.kcanimal.or.kr/board_gallery01/board_list.asp"
BASE_DOMAIN = "https://www.kcanimal.or.kr" 

DB_TABLE_NAME = "ANIMALS"

# ğŸ’¡ DB í…Œì´ë¸” ì»¬ëŸ¼ ëª©ë¡ (ì´ 14ê°œ ì»¬ëŸ¼: BOARD_IDX í¬í•¨)
ANIMAL_COLUMNS = ["BOARD_IDX", "NAME", "SPECIES", "BREED", "GENDER", "FEATURE", "PHOTO1", "PHOTO2", "PHOTO3", 
                 "RESCUE_DATE", "RESCUE_LOCATION", "AGE", "CRAWL_URL", "LAST_CRAWLED_AT"]

# ğŸ’¡ UPSERTë¥¼ ìœ„í•œ ê³ ìœ  í‚¤ (BOARD_IDXë§Œ ì‚¬ìš©)
UNIQUE_KEY_COLUMNS = ["BOARD_IDX"]
UNIQUE_KEY_NAME = "unique_animal_record_v9_idx" # í‚¤ ì´ë¦„ ë³€ê²½
ITEMS_PER_PAGE = 12 


# ====================================================================
# 2. ë°ì´í„° íŒŒì‹± ë„ìš°ë¯¸ í•¨ìˆ˜ 
# ====================================================================
def parse_date(date_str):
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
    if match:
        date_str = match.group(1)
    try:
        return datetime.strptime(date_str, '%Y-%m-%d').date() 
    except ValueError:
        return date.today()
        
def parse_age(age_str):
    """
    ë‚˜ì´ ë¬¸ìì—´ì„ ë¶„ì„í•˜ì§€ ì•Šê³ , ì•ë’¤ ê³µë°±ë§Œ ì œê±°í•œ í›„ ì›ë³¸ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    cleaned_age_str = age_str.strip()
    
    if not cleaned_age_str:
        return 'ë¯¸ìƒ'
        
    return cleaned_age_str

# ====================================================================
# 3. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ (ì‚¬ì§„ URL ì¶”ì¶œ ë¡œì§ ì¬ê°•í™”)
# ====================================================================
def fetch_detail_info(board_idx):
    if not board_idx or not str(board_idx).isdigit():
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "board_idx ì˜¤ë¥˜", None, None, None
        
    detail_url = f"{BASE_DOMAIN}/board_gallery01/board_content.asp?board_idx={board_idx}&tname=board_gallery01"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(detail_url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        # ì¶•ì¢…/í’ˆì¢… ì¶”ì¶œ
        species_th = soup.find("th", text="ì¶•ì¢…")
        species = species_th.find_next_sibling('td').text.strip() if species_th else "ë¯¸ìƒ"
        
        breed_th = soup.find("th", text="í’ˆì¢…")
        breed = breed_th.find_next_sibling('td').text.strip() if breed_th else "ë¯¸ìƒ"
        if breed == "-": breed = "ë¯¸ìƒ" 

        # ğŸ’¡ ì‚¬ì§„ URL 3ê°œ ì¶”ì¶œ ë¡œì§ ğŸ’¡
        photo_urls = []
        selectors = [
            '.board_content_img img', 
            'td[colspan="4"] img',
            'div.board_content_img_box img' 
        ]

        for selector in selectors:
            for img in soup.select(selector):
                src = img.get('src', '').strip()
                if src:
                    if src.startswith('/'):
                        full_url = BASE_DOMAIN + src
                    elif src.startswith('http'):
                        full_url = src
                    else:
                        continue 
                    
                    if full_url not in photo_urls:
                        photo_urls.append(full_url)
                
                if len(photo_urls) >= 3:
                    break
            
            if len(photo_urls) >= 3:
                break
        
        photo1 = photo_urls[0] if len(photo_urls) > 0 else None
        photo2 = photo_urls[1] if len(photo_urls) > 1 else None
        photo3 = photo_urls[2] if len(photo_urls) > 2 else None


        # íŠ¹ì§• ë° íŠ¹ì´ì‚¬í•­ ì¶”ì¶œ -> Featureë¡œ í†µí•©
        features = []
            
        feature_th = soup.find("th", text="íŠ¹ì§•")
        feature_text = feature_th.find_next_sibling('td').text.strip() if feature_th else ""
        if feature_text: features.append(f"íŠ¹ì§•:{feature_text}")
            
        special_th = soup.find("th", text="íŠ¹ì´ì‚¬í•­")
        if special_th:
            special_text = special_th.find_next_sibling('td').text.strip()
            if special_text: features.append(f"íŠ¹ì´ì‚¬í•­:{special_text}")
                
        final_feature_detail = ", ".join(features)
        
        return species, breed, final_feature_detail, photo1, photo2, photo3

    except requests.exceptions.RequestException:
        print(f" Â [Fail] ìƒì„¸ ìš”ì²­ ì‹¤íŒ¨: {detail_url} (ë„¤íŠ¸ì›Œí¬/ì„œë²„ ì˜¤ë¥˜)")
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "ìƒì„¸ ìš”ì²­ ì˜¤ë¥˜", None, None, None 
    except Exception as e:
        print(f" Â [Fail] íŒŒì‹± ì‹¤íŒ¨: {detail_url} ({e})")
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜", None, None, None

# ====================================================================
# 4. ë™ì  í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ ë° ëª©ë¡ í¬ë¡¤ë§ í•¨ìˆ˜ 
# ====================================================================
def get_total_pages(url):
    """ì´ ê²Œì‹œë¬¼ ìˆ˜ë¥¼ íŒŒì‹±í•˜ì—¬ ì´ í˜ì´ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (íŒŒì‹± ì•ˆì •ì„± ê°•í™”)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')
        
        # ì´ ê²Œì‹œë¬¼ ìˆ˜ ì„ íƒì ê°•í™”
        total_count_element = soup.select_one("td.list_total strong")
        total_items = 0 
        
        if total_count_element:
            match = re.search(r'(\d+)', total_count_element.text)
            if match:
                total_items = int(match.group(1))
        
        if total_items == 0:
            text_element = soup.find(text=re.compile(r'\d+\s*ê±´'))
            if text_element:
                match = re.search(r'(\d+)', text_element)
                if match:
                    total_items = int(match.group(1))
            
        if total_items == 0:
            print("[Warning] ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 1í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
            return 1 
        else:
            total_pages = math.ceil(total_items / ITEMS_PER_PAGE)
        
        print(f" Â  Â [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì´ ê²Œì‹œë¬¼ ìˆ˜: {total_items}ê°œ, í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜: {ITEMS_PER_PAGE}ê°œ")
        print(f" Â  Â [DEBUG] ê³„ì‚°ëœ ì´ í˜ì´ì§€ ìˆ˜: {total_pages}ê°œ")
        
        return total_pages

    except requests.exceptions.RequestException:
        print(f" Â [Error] ì´ í˜ì´ì§€ ìˆ˜ ìš”ì²­ ì˜¤ë¥˜. 1í˜ì´ì§€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f" Â [Error] í˜ì´ì§€ ìˆ˜ íŒŒì‹± ì˜¤ë¥˜: {e}. 1í˜ì´ì§€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        return 1


def fetch_data(url):
    """ì§€ì •ëœ URLì—ì„œ ë™ë¬¼ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  ìƒì„¸ í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. 
    ë°˜í™˜ ê°’ì— board_idxì™€ ìƒì„¸ í˜ì´ì§€ URLì„ í¬í•¨í•©ë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        # ëª©ë¡ í•­ëª© ì„ íƒì ëŒ€í­ ê°•í™” 
        items = soup.select("ul.list_gallery_ul > li, #goodsBox > ul > li, .board_list_gallery > ul > li") 
            
        print(f" Â  Â [DEBUG] URL: {url} | ë°œê²¬ëœ í•­ëª© ìˆ˜: {len(items)}ê°œ")
        
        data = []
        
        current_page_url = url # ëª©ë¡ í˜ì´ì§€ URL (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            def process_item(item):
                board_idx = "N/A"
                try:
                    link = item.select_one('a')
                    if not link or not link.get('href'): return None
                    
                    # ìƒì„¸ URLì—ì„œ board_idx ì¶”ì¶œ
                    detail_href = link['href']
                    match_idx = re.search(r'board_idx=(\d+)', detail_href)
                    if not match_idx: return None 
                    board_idx = match_idx.group(1) 
                    
                    # CRAWL_URLì— ì €ì¥í•  ìƒì„¸ URLì„ ìƒì„±
                    detail_url_to_save = f"{BASE_DOMAIN}/board_gallery01/board_content.asp?board_idx={board_idx}&tname=board_gallery01"
                    
                    # ëª©ë¡ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    p_text_el = item.select_one("div p")
                    if not p_text_el: return None
                    p_text = p_text_el.text.strip()
                    
                    match_name = re.match(r'(.+?)\s*\(\d{2}-\d+\)', p_text)
                    # ğŸ’¡ [ìˆ˜ì •ëœ ë¶€ë¶„] ì´ë¦„ì´ ì—†ìœ¼ë©´ "(ì´ë¦„ì—†ìŒ)"ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•­ëª©ì„ ë²„ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    name = match_name.group(1).strip() if match_name and match_name.group(1).strip() else "(ì´ë¦„ì—†ìŒ)"
                    
                    # ğŸ’¡ ì´ë¦„ì´ "(ì´ë¦„ì—†ìŒ)"ì¸ ê²½ìš°ì—ë„ í•­ëª©ì„ ë²„ë¦¬ì§€ ì•Šë„ë¡ í•´ë‹¹ `if` êµ¬ë¬¸ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.

                    feature_status = p_text.split(')')[-1].strip()
                    
                    span_text_el = item.select_one("div span")
                    if not span_text_el: return None
                    span_text = span_text_el.text.strip().split('|')

                    rescue_loc = span_text[0].strip() if len(span_text) > 0 else "ë¯¸ìƒ"
                    rescue_date_str = span_text[1].strip() if len(span_text) > 1 else "ë¯¸ìƒ"
                    age_str = span_text[2].strip() if len(span_text) > 2 else "0ì‚´"
                    gender = span_text[3].strip() if len(span_text) > 3 else "ë¯¸ìƒ"
                    weight = span_text[4].strip() if len(span_text) > 4 else "0kg"
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    species, breed, feature_detail, photo1, photo2, photo3 = fetch_detail_info(board_idx) 
                    
                    # í•„ìˆ˜ ë°ì´í„° ìœ íš¨ì„± ì¬í™•ì¸ (ì´ë¦„ ì œì™¸)
                    # ì¶•ì¢…, êµ¬ì¡°ì¼, í’ˆì¢… ì •ë³´ëŠ” ë°˜ë“œì‹œ ìˆì–´ì•¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
                    if species == "ë¯¸ìƒ" or rescue_date_str == "ë¯¸ìƒ" or breed == "ë¯¸ìƒ":
                        return None 

                    rescue_date = parse_date(rescue_date_str)
                    age = parse_age(age_str)
                    feature = f"ìƒíƒœ:{feature_status}, ë¬´ê²Œ:{weight}, ìƒì„¸íŠ¹ì§•:[{feature_detail}]"
                    
                    # ìµœì¢… ë°ì´í„° ë¦¬í„´ (ì´ë¦„ì´ ì—†ì–´ë„ ì €ì¥ë¨)
                    return (board_idx, name, species, breed, gender, feature, photo1, photo2, photo3, 
                            rescue_date, rescue_loc, age, detail_url_to_save)
                            
                except Exception as item_e:
                    # print(f" Â  Â [Fail] ëª©ë¡ í•­ëª© íŒŒì‹± ì‹¤íŒ¨ (idx:{board_idx}): {item_e}")
                    return None

            results = executor.map(process_item, items)
            data = [result for result in results if result is not None]

        return data
        
    except requests.exceptions.RequestException as e:
        print(f" Â [Error] ì›¹ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f" Â [Error] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        return []

# ====================================================================
# 5. DB ë° ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜ 
# ====================================================================

def initialize_db_schema():
    """DBì— ì—°ê²°í•˜ì—¬ UPSERTë¥¼ ìœ„í•œ UNIQUE KEYê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
    print("ğŸ› ï¸ DB ì´ˆê¸°í™” (UNIQUE KEY ì„¤ì •)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        key_columns_str = ', '.join(f'`{c}`' for c in UNIQUE_KEY_COLUMNS)
        sql_add_unique_key = f"""
        ALTER TABLE {DB_TABLE_NAME}
        ADD UNIQUE KEY {UNIQUE_KEY_NAME} ({key_columns_str});
        """
        
        # ê¸°ì¡´ UNIQUE KEY ì‚­ì œ ì‹œë„ (ì•ˆì •ì„± ê°•í™”)
        try:
            print(" Â [DEBUG] ê¸°ì¡´ UNIQUE KEY ì‚­ì œ ì‹œë„...")
            # ì´ì „ ë²„ì „ì˜ í‚¤ ì‚­ì œ ì‹œë„
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_num_id;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_no_breed;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_v6;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_test;")
            # í˜„ì¬ í‚¤ë„ í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ëŒ€ë¹„ ì‚­ì œ ì‹œë„
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY {UNIQUE_KEY_NAME};") 
            conn.commit()
            print(" Â [DEBUG] ì´ì „ UNIQUE KEY ì‚­ì œ ì™„ë£Œ.")
        except pymysql.err.ProgrammingError as e:
             if e.args[0] != 1091: # 1091: KEYê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
                 print(f" Â [DEBUG] ì´ì „ KEY ì‚­ì œ ì‹¤íŒ¨: {e}")
             pass 
        except Exception:
             pass 

        # ìƒˆë¡œìš´ UNIQUE KEY ì„¤ì •
        curs.execute(sql_add_unique_key)
        conn.commit()
        print(f"âœ… UNIQUE KEY '{UNIQUE_KEY_NAME}' ì„¤ì • ì™„ë£Œ: ({key_columns_str})")

    except pymysql.err.ProgrammingError as e:
        if e.args[0] == 1061: 
            print(f"âš ï¸ UNIQUE KEY '{UNIQUE_KEY_NAME}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. (ì´ˆê¸°í™” ê±´ë„ˆë›°ê¸°)")
        elif e.args[0] == 1146:
            print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: í…Œì´ë¸” '{DB_TABLE_NAME}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í…Œì´ë¸” ìƒì„± í•„ìš”)")
        elif e.args[0] == 1072: # Key column 'BOARD_IDX' doesn't exist in table
             print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: í…Œì´ë¸”ì— BOARD_IDX ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì„¸ìš”! ({e})")
        else:
            print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"âŒ DB ì—°ê²°/ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if curs: curs.close()
        if conn: conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ.")


def job_crawl_and_save():
    job_timestamp = datetime.now() 
    current_time = job_timestamp.strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n=======================================================")
    print(f"ğŸš€ ìŠ¤ì¼€ì¤„ë§ëœ ë™ë¬¼ ë°ì´í„° í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: {current_time}")
    print(f"=======================================================")

    total_pages = get_total_pages(CRAWL_URL)

    if total_pages == 0:
        print("[INFO] ì´ í˜ì´ì§€ ìˆ˜ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    urls = []
    # ğŸ’¡ í˜ì´ì§€ ì œí•œì„ total_pagesë¡œ ë³€ê²½í•˜ì—¬ ì „ì²´ í¬ë¡¤ë§ 
    max_pages_to_crawl = total_pages
    
    for page in range(1, max_pages_to_crawl + 1): 
        if page == 1:
            urls.append(CRAWL_URL) 
        else:
            urls.append(f"{CRAWL_URL}?page={page}")
            
    print(f"[INFO] í¬ë¡¤ë§í•  ìµœì¢… URL ìˆ˜: {len(urls)}ê°œ (ì´ {total_pages}í˜ì´ì§€ ëª¨ë‘ ì‹œë„)")
    
    all_data = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_data, urls)
        for result in results:
            all_data.extend(result)
            
    # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±°
    data_list = list(set(tuple(row) for row in all_data))
    
    print(f"[INFO] í¬ë¡¤ë§ëœ ìœ íš¨ ë°ì´í„° í•­ëª© ìˆ˜ (ì¤‘ë³µ ì œê±° í›„): {len(data_list)}ê°œ")
    
    if not data_list:
        print("âš ï¸ í¬ë¡¤ë§ëœ ìœ íš¨ ë°ì´í„°ê°€ ì—†ì–´ DB ì‘ì—…ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 3. ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° CSV ì €ì¥
    # ğŸ’¡ í¬ë¡¤ë§ëœ ë°ì´í„°(13ê°œ: BOARD_IDX + 11ê°œ í•­ëª© + CRAWL_URL)ì— íƒ€ì„ìŠ¤íƒ¬í”„(1ê°œ)ë¥¼ ì¶”ê°€í•˜ì—¬ 14ê°œ ì»¬ëŸ¼ì— ë§ì¶¤
    df = pd.DataFrame([row + (job_timestamp,) for row in data_list], columns=ANIMAL_COLUMNS) 
    timestamp_str = job_timestamp.strftime("%Y%m%d_%H%M%S")
    csv_filename = f"{DB_TABLE_NAME}_{timestamp_str}.csv"
    
    try:
        df.to_csv(csv_filename, header=True, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')
        print(f"âœ… Data saved successfully to {csv_filename}")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 4. MySQL ì—°ê²° ë° ì €ì¥ (UPSERT & DELETE)
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        # 4.1. DB UPSERT ì¿¼ë¦¬ ìƒì„±
        column_names = ANIMAL_COLUMNS
        value_placeholders = ', '.join(['%s'] * len(column_names))
        
        # LAST_CRAWLED_ATê³¼ CRAWL_URL, NAME, SPECIES ë“± ë¹„-ê³ ìœ  í‚¤ ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸
        update_cols = [
            f'`{c}` = VALUES(`{c}`)' 
            for c in column_names 
            if c not in UNIQUE_KEY_COLUMNS # BOARD_IDXë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
        ]
        update_set_clause = ', '.join(update_cols)
        
        sql_upsert = f"""
        INSERT INTO {DB_TABLE_NAME} ({', '.join(f'`{c}`' for c in column_names)}) 
        VALUES({value_placeholders})
        ON DUPLICATE KEY UPDATE
            {update_set_clause};
        """ 
        
        data_to_insert = [tuple(row) for row in df.values]
        
        rows_processed = curs.executemany(sql_upsert, data_to_insert)
        
        conn.commit()
        
        print(f"âœ… DB UPSERT ì™„ë£Œ. ì´ {rows_processed}ê°œ ë ˆì½”ë“œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤ (ì‚½ì…/ì—…ë°ì´íŠ¸ í¬í•¨).")
        
        # 4.2. ì‚¬ë¼ì§„ ë°ì´í„° ì‚­ì œ (DELETE)
        sql_delete_old = f"""
        DELETE FROM {DB_TABLE_NAME} 
        WHERE LAST_CRAWLED_AT < %s;
        """
        rows_deleted = curs.execute(sql_delete_old, (job_timestamp,))
        
        conn.commit()
        
        print(f"âœ… ì‚¬ë¼ì§„ ë°ì´í„° ì‚­ì œ ì™„ë£Œ. ì´ {rows_deleted}ê°œ ë ˆì½”ë“œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ DB ì‘ì—… ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if conn:
            conn.rollback()
            print("âŒ DB ë¡¤ë°± ì™„ë£Œ.")
            
    finally:
        if curs: curs.close()
        if conn: conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ.")

# ====================================================================
# 6. ìŠ¤ì¼€ì¤„ ì„¤ì • ë° ì‹¤í–‰ ë£¨í”„ 
# ====================================================================

if __name__ == '__main__':
    # ğŸ’¡ ì‹¤í–‰ ì „ MySQL í…Œì´ë¸”ì— BOARD_IDX ì»¬ëŸ¼ì´ ì¶”ê°€ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°, AGE ì»¬ëŸ¼ì€ VARCHAR íƒ€ì…ìœ¼ë¡œ ë³€ê²½ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤!
    initialize_db_schema() 
    
    # ìŠ¤ì¼€ì¤„ ê°„ê²© 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
    schedule.every(30).minutes.do(job_crawl_and_save) 

    print("=======================================================")
    print(f"Scheduler í™œì„±í™”ë¨. 1ë¶„ë§ˆë‹¤ ì‘ì—…ì„ í™•ì¸í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.")
    print("=======================================================")

    # ìµœì´ˆ 1íšŒ ì‹¤í–‰
    job_crawl_and_save()

    while True:
        schedule.run_pending()
        time.sleep(10)