import pymysql.cursors
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import csv
import time
import schedule
from datetime import datetime
from datetime import date 
import re

# ====================================================================
# 1. í™˜ê²½ ì„¤ì • (DB ì ‘ì† ì •ë³´ ë° ê¸°íƒ€ ì„¤ì •)
# ====================================================================
# MySQL ì ‘ì† ì •ë³´ (ì´ë¯¸ì§€: image_c00623.png, image_c0063f.png ê¸°ë°˜)
DB_CONFIG = {
    "host": "project-db-campus.smhrd.com",
    "port": 3307, # ğŸ’¡ í¬íŠ¸ 3307 ëª…ì‹œ (image_c0063f.png)
    "user": "campus_24IS_CLOUD3_p3_1",
    "password": "smhrd1", 
    "database": "campus_24IS_CLOUD3_p3_1",
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

# í¬ë¡¤ë§í•  ëŒ€ìƒ URL (ìœ ê¸°ë™ë¬¼ ê³µê³  ê°¤ëŸ¬ë¦¬ ëª©ë¡)
CRAWL_URL = "https://www.kcanimal.or.kr/board_gallery01/board_list.asp"
BASE_DOMAIN = "https://www.kcanimal.or.kr" 

# DBì— ì €ì¥í•  í…Œì´ë¸” ì»¬ëŸ¼ëª… (DBì™€ ìˆœì„œ ì¼ì¹˜ í•„ìˆ˜)
ANIMAL_COLUMNS = ["NAME", "SPECIES", "GENDER", "FEATURE", "PHOTO", 
                  "RESCUE_DATE", "RESCUE_LOCATION", "AGE"]

# UPSERTì˜ ê¸°ì¤€ì´ ë˜ëŠ” UNIQUE KEY ì»¬ëŸ¼
UNIQUE_KEY_COLUMNS = ["NAME", "SPECIES", "RESCUE_DATE"]
UNIQUE_KEY_NAME = "unique_animal_record"


# ====================================================================
# 2. ë°ì´í„° íŒŒì‹± ë„ìš°ë¯¸ í•¨ìˆ˜
# ====================================================================

def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime.date ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
    if match:
        date_str = match.group(1)
    
    try:
        return datetime.strptime(date_str, '%Y-%m-%d').date() 
    except ValueError:
        return date.today()
        
def parse_age(age_str):
    """ë‚˜ì´ ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ intë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ('ë…„ìƒ', 'ê°œì›”', 'ì£¼' í¬í•¨ ì‹œ 0 ë°˜í™˜)"""
    age_str = age_str.strip()
    
    if 'ë…„ìƒ' in age_str or 'ê°œì›”' in age_str or 'ì£¼' in age_str:
        return 0 

    match = re.search(r'(\d+)\s*ì‚´', age_str) 
    if match:
        return int(match.group(1))

    return 0

# ====================================================================
# 3. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ (SPECIES, FEATURE ì¶”ì¶œ ê°œì„ )
# ====================================================================

def fetch_detail_species(detail_url):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ í’ˆì¢…(SPECIES), íŠ¹ì§•(FEATURE), íŠ¹ì´ì‚¬í•­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    if not detail_url.startswith("http"):
        # BASE_DOMAINê³¼ detail_urlì„ ì•ˆì „í•˜ê²Œ ê²°í•©
        detail_url = BASE_DOMAIN + detail_url 
        
    try:
        # User-Agent í—¤ë” ì¶”ê°€ (fetch_dataì™€ ë™ì¼í•˜ê²Œ ì ìš©)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(detail_url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        # 1. í’ˆì¢…(SPECIES) ì¶”ì¶œ (ì¶•ì¢…ê³¼ í’ˆì¢… ëª¨ë‘ ê³ ë ¤: image_c06815.png)
        species_th = soup.select_one("th:contains('ì¶•ì¢…')")
        species = species_th.find_next_sibling('td').text.strip() if species_th else "í’ˆì¢…ë¯¸ìƒ"
        
        species_detail_th = soup.select_one("th:contains('í’ˆì¢…')")
        species_detail = species_detail_th.find_next_sibling('td').text.strip() if species_detail_th else ""
        
        # ìƒì„¸ ì¢…ë¥˜(í’ˆì¢…)ì´ ìˆë‹¤ë©´ ì´ë¥¼ ìµœì¢… SPECIESë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if species_detail and species_detail != "-":
            species = species_detail 
            
        # 2. íŠ¹ì§• (íŠ¹ì§•, íŠ¹ì´ì‚¬í•­) ì¶”ì¶œ (image_c06815.png, image_c06838.png)
        features = []
        
        # 'íŠ¹ì§•' ì¶”ì¶œ
        feature_th = soup.select_one("th:contains('íŠ¹ì§•')")
        feature_text = feature_th.find_next_sibling('td').text.strip() if feature_th else ""
        if feature_text:
            features.append(f"íŠ¹ì§•:{feature_text}")

        # 'íŠ¹ì´ì‚¬í•­' ì¶”ì¶œ
        special_th = soup.select_one("th:contains('íŠ¹ì´ì‚¬í•­')")
        if special_th:
            special_text = special_th.find_next_sibling('td').text.strip()
            if special_text:
                 features.append(f"íŠ¹ì´ì‚¬í•­:{special_text}")

        final_feature_detail = ", ".join(features)
        
        return species, final_feature_detail

    except requests.exceptions.RequestException as e:
        return "í’ˆì¢…ë¯¸ìƒ", ""
    except Exception as e:
        return "í’ˆì¢…ë¯¸ìƒ", ""

# ====================================================================
# 4. ëª©ë¡ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜ (User-Agent í—¤ë” ì¶”ê°€)
# ====================================================================

def fetch_data(url):
    """
    ì§€ì •ëœ URLì—ì„œ ë™ë¬¼ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¢… ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        # ğŸ’¡ User-Agent í—¤ë” ì¶”ê°€ (í¬ë¡¤ë§ ì‹¤íŒ¨(í•­ëª© ìˆ˜ 0ê°œ) ë°©ì§€: image_c01508.png)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        items = soup.select("#goodsBox > ul > li") 
        print(f"    [DEBUG] ë°œê²¬ëœ í•­ëª© ìˆ˜: {len(items)}ê°œ")
        
        data = []
        
        # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=5) as executor:
            def process_item(item):
                try:
                    link = item.select_one('a')
                    if not link:
                        return None
                    
                    # --- ëª©ë¡ í¬ë¡¤ë§ ---
                    p_text = item.select_one("div p").text.strip()
                    match_name = re.match(r'(.+?)\s*\(\d{2}-\d+\)', p_text)
                    name = match_name.group(1).strip() if match_name and match_name.group(1).strip() else "(ì´ë¦„ì—†ìŒ)"
                    feature_status = p_text.split(')')[-1].strip()

                    span_text = item.select_one("div span").text.strip().split('|')

                    rescue_loc = span_text[0].strip()
                    rescue_date_str = span_text[1].strip()
                    age_str = span_text[2].strip()
                    gender = span_text[3].strip()
                    weight = span_text[4].strip()
                    
                    photo_url = BASE_DOMAIN + item.select_one("img")['src'].strip() 
                    detail_url = link['href'] 

                    # --- 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (SPECIES, FEATURE) ---
                    species_detail, feature_detail = fetch_detail_species(detail_url)

                    # --- 3. ë°ì´í„° ì •ì œ ë° íŠœí”Œ ìƒì„± ---
                    rescue_date = parse_date(rescue_date_str)
                    age = parse_age(age_str)
                    
                    # FEATURE: ëª©ë¡ ìƒíƒœ ì •ë³´ + ë¬´ê²Œ + ìƒì„¸ í˜ì´ì§€ íŠ¹ì§• ì •ë³´ ê²°í•©
                    feature = f"ìƒíƒœ:{feature_status}, ë¬´ê²Œ:{weight}, ìƒì„¸íŠ¹ì§•:[{feature_detail}]"
                    
                    return (name, species_detail, gender, feature, photo_url, 
                            rescue_date, rescue_loc, age)
                            
                except Exception:
                    return None

            results = executor.map(process_item, items)
            data = [result for result in results if result is not None]

        return data
        
    except requests.exceptions.RequestException as e:
        print(f" Â [Error] ì›¹ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f" Â [Error] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        return []

# ====================================================================
# 5. DB ë° ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜ 
# ====================================================================

def initialize_db_schema():
    """DBì— ì—°ê²°í•˜ì—¬ UPSERTë¥¼ ìœ„í•œ UNIQUE KEYê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
    print("ğŸ› ï¸ DB ì´ˆê¸°í™” (UNIQUE KEY ì„¤ì •)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        key_columns_str = ', '.join(f'`{c}`' for c in UNIQUE_KEY_COLUMNS)
        sql_add_unique_key = f"""
        ALTER TABLE ANIMALS
        ADD UNIQUE KEY {UNIQUE_KEY_NAME} ({key_columns_str});
        """
        
        curs.execute(sql_add_unique_key)
        conn.commit()
        print(f"âœ… UNIQUE KEY '{UNIQUE_KEY_NAME}' ì„¤ì • ì™„ë£Œ: ({key_columns_str})")

    except pymysql.err.ProgrammingError as e:
        # 1061 ì—ëŸ¬ ì½”ë“œëŠ” í‚¤ê°€ ì´ë¯¸ ì¡´ì¬í•¨ì„ ì˜ë¯¸ (image_c07686.png)
        if e.args[0] == 1061: 
            print(f"âš ï¸ UNIQUE KEY '{UNIQUE_KEY_NAME}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. (ì´ˆê¸°í™” ê±´ë„ˆë›°ê¸°)")
        else:
            print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            print("â— 'animals' í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ DB ì—°ê²°/ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if curs:
            curs.close()
        if conn:
            conn.close()
            print("âœ… DB ì—°ê²° ì¢…ë£Œ.")

def job_crawl_and_save():
    """ë©”ì¸ ì‘ì—… í•¨ìˆ˜: í¬ë¡¤ë§ -> ë°ì´í„° ì •ì œ -> DB ì €ì¥ (UPSERT)"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n=======================================================")
    print(f"ğŸš€ ìŠ¤ì¼€ì¤„ë§ëœ ë™ë¬¼ ë°ì´í„° í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: {current_time}")
    print(f"=======================================================")

    # 1. í¬ë¡¤ë§ ë°ì´í„° ìˆ˜ì§‘ (fetch_data í•¨ìˆ˜ í˜¸ì¶œ)
    urls = [CRAWL_URL] 
    all_data = []
    
    results = map(fetch_data, urls)
    for result in results:
        all_data.extend(result)
            
    data_list = list(set(all_data))
    
    if not data_list:
        print("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ì–´ DB ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„° ì²˜ë¦¬ ë° CSV ì €ì¥
    df = pd.DataFrame(data_list, columns=ANIMAL_COLUMNS)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"animals_{timestamp_str}.csv"
    
    try:
        df.to_csv(csv_filename, header=True, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')
        print(f"âœ… Data saved successfully to {csv_filename}")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 3. MySQL ì—°ê²° ë° ì €ì¥ (UPSERT)
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        # 3.1. DB UPSERT ì¿¼ë¦¬ ìƒì„±
        column_names = ANIMAL_COLUMNS
        value_placeholders = ', '.join(['%s'] * len(column_names))
        
        update_cols = [
            f'`{c}` = VALUES(`{c}`)' 
            for c in column_names 
            if c not in UNIQUE_KEY_COLUMNS
        ]
        update_set_clause = ', '.join(update_cols)
        
        sql_upsert = f"""
        INSERT INTO ANIMALS ({', '.join(f'`{c}`' for c in column_names)}) 
        VALUES({value_placeholders})
        ON DUPLICATE KEY UPDATE
            {update_set_clause};
        """ 
        
        data_to_insert = [tuple(row) for row in df.values]
        
        rows_processed = curs.executemany(sql_upsert, data_to_insert)
          
        conn.commit()
        
        print(f"âœ… DB UPSERT ì™„ë£Œ. ì´ {rows_processed}ê°œ ë ˆì½”ë“œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤ (ì‚½ì…/ì—…ë°ì´íŠ¸ í¬í•¨).")

    except Exception as e:
        print(f"âŒ DB ì‘ì—… ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if conn:
            conn.rollback()
            print("âŒ DB ë¡¤ë°± ì™„ë£Œ.")
            
    finally:
        if curs:
            curs.close()
        if conn:
            conn.close()
            print("âœ… DB ì—°ê²° ì¢…ë£Œ.")

# ====================================================================
# 6. ìŠ¤ì¼€ì¤„ ì„¤ì • ë° ì‹¤í–‰ ë£¨í”„
# ====================================================================

if __name__ == '__main__':
    initialize_db_schema() 
    
    # âš ï¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë§¤ 1ë¶„ë§ˆë‹¤ ì‹¤í–‰ (í•„ìš”ì— ë”°ë¼ ì£¼ê¸°ë¥¼ ì¡°ì •í•˜ì„¸ìš”)
    schedule.every(1).minutes.do(job_crawl_and_save) 
    print("=======================================================")
    print(f"Scheduler í™œì„±í™”ë¨. ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ì£¼ê¸°ì ìœ¼ë¡œ ì‘ì—…ì„ í™•ì¸í•©ë‹ˆë‹¤.")
    print("=======================================================")

    while True:
        schedule.run_pending()
        time.sleep(10)