# -*- coding: utf-8 -*-
import base64
import glob
import json
import re
import numpy as np
import os
import sys  # â—€â—€ (ì¶”ê°€) í„°ë¯¸ë„ ì¸ìë¥¼ ë°›ê¸° ìœ„í•´ import
import time # â—€â—€ (ì¶”ê°€) ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ import
import io

import boto3
from openai import OpenAI
import faiss  # â—€â—€ (ì¶”ê°€) FAISS import
import re

# --- (ì‹ ê·œ) â—€â—€ ì „ì—­ ìƒìˆ˜ ì„¤ì • ---
VECTOR_DIMENSION = 3072
K_CANDIDATES = 100 # FAISS ì˜ˆì„  í›„ë³´ ìˆ˜
K_FINAL = 10       # ìµœì¢… ê²°ê³¼ ìˆ˜

DB_FILE = "./dog_cat_features_attr_emb.json"
ID_MAP_FILE = "id_map.json"
INDEX_FILE = "animal_vectors.index"

# --- 0. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    # 1. OpenAI í‚¤ ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    with open('./API-Key.txt','r') as f:
        os.environ['OPENAI_API_KEY'] = f.read().strip()
    client = OpenAI()
    
    # 2. NCP í‚¤/ë²„í‚· ì •ë³´ ë¡œë“œ (í™˜ê²½ ë³€ìˆ˜ë¡œë§Œ ì„¤ì •)
    with open('./ACCESS_KEY.txt','r') as f:
        os.environ['NCP_ACCESS_KEY'] = f.read().strip()
        
    # (ìˆ˜ì •ë¨) â—€â—€ ë³€ìˆ˜ëª… `api_key` ë®ì–´ì“°ê¸° ë²„ê·¸ ìˆ˜ì •
    with open('./SECRET_KEY.txt','r') as f:
        ncp_secret_key = f.read().strip() # â—€ 'api_key'ê°€ ì•„ë‹Œ ìƒˆ ë³€ìˆ˜ëª… ì‚¬ìš©
        
    # (ìˆ˜ì •ë¨) â—€â—€ í•˜ë“œì½”ë”©ëœ ë³€ìˆ˜ ëŒ€ì‹  íŒŒì¼ì—ì„œ ì½ì€ 'ncp_secret_key' ì‚¬ìš©
    os.environ['NCP_SECRET_KEY'] = ncp_secret_key
    
    # 3. NCP ì „ì—­ ì„¤ì • (boto3ê°€ ì‚¬ìš©í•  ì •ë³´)
    endpoint_url = "https://kr.object.ncloudstorage.com"
    region_name = "kr-standard"
    bucket_name = "animal-bucket"

except Exception as e:
    print(f"âŒ í‚¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. (API-Key.txt, ACCESS_KEY.txt, SECRET_KEY.txt, BUCKET_NAME.txtê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”): {e}")
    sys.exit()

# --- 1. í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
prompt = """
ë°˜ë“œì‹œ ì•„ë˜ JSON êµ¬ì¡° ì™¸ì˜ ì–´ë–¤ ë§ë„ í•˜ì§€ ë§ˆë¼.
ì‘ë‹µì´ JSON ì™¸ì˜ ë¬¸ìë¥¼ í¬í•¨í•˜ë©´ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬ëœë‹¤.
ë„ˆëŠ” ì´ë¯¸ì§€ ì† ë™ë¬¼ì˜ ì™¸í˜•ì  íŠ¹ì§•ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ë‹¤.
í’ˆì¢… ìœ ì‚¬ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ê°œì²´ì˜ ì„¸ë°€í•œ íŠ¹ì§•(í„¸ íŒ¨í„´, ê·€ ê°ë„, ì²´í˜• ë¹„ìœ¨, ëˆˆ í¬ê¸° ë“±)ì„ ì •í™•íˆ ê¸°ìˆ í•˜ë¼.
ë°˜ë“œì‹œ ëª¨ë“  ì‹œê°ì  íŠ¹ì§•ì„ ê°€ëŠ¥í•œ í•œ ì •ì œëœ í˜•ìš©ì‚¬ë¡œ ê¸°ìˆ í•˜ë¼.
ë‹¨ìˆœíˆ ìƒ‰ìƒëª…ì´ë‚˜ í˜•íƒœëª…ë§Œ ê¸°ì…í•˜ì§€ ë§ê³ , êµ¬ì²´ì  ì§ˆê°Â·ëª…ë„Â·íŒ¨í„´Â·ìƒëŒ€ì  ë¹„ìœ¨ì„ í•¨ê»˜ ì„œìˆ í•˜ë¼.
ì´ë¯¸ì§€ì—ì„œ ê´€ì°°í•  ìˆ˜ ìˆëŠ” ì„¸ë¶€ì ì¸ ì™¸í˜• íŠ¹ì§•ì„ ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œ **ì •í™•íˆ** ì¶œë ¥í•˜ë¼.
ë‹¤ë¥¸ ì´ë¯¸ì§€ì™€ ë¹„êµí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ì˜ˆì •ì´ë¯€ë¡œ,
ë™ì¼í•œ ì†ì„±ì— ëŒ€í•´ì„œëŠ” ì¼ê´€ëœ ì–´íœ˜ ì²´ê³„ë¥¼ ìœ ì§€í•˜ë¼.
ì˜ˆ: 'ì§§ì€ í„¸', 'ì§§ìŒ', 'ì§§ì€ ëª¨'ëŠ” ëª¨ë‘ 'ì§§ì€ í„¸'ë¡œ í†µì¼í•˜ë¼.

ì£¼ì˜:
- JSON ì™¸ì˜ ë¬¸ì¥ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆë¼.
- ëª¨ë“  í•„ë“œëŠ” ë°˜ë“œì‹œ ì±„ì›Œë¼.
- **í’ˆì¢… ì¶”ì •ì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤** ëª¨ë“  ì‹œê°ì  ì¦ê±°(ë¨¸ë¦¬ ëª¨ì–‘, ê·€ ëª¨ì–‘, ê·€ ì„¸íŠ¸, í„¸, ì‹ ì²´ êµ¬ì¡°)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ 1ì°¨ í’ˆì¢…ì„ ì¶”ì¸¡í•˜ë¼
- í’ˆì¢… ì¶”ì •ì˜ í™•ë¥ ì´ ë„ˆë¬´ ë‚®ì€ ê²½ìš° 'ë¯¹ìŠ¤'ë¡œ í‘œê¸°í•˜ë¼
- ë‹¨ìœ„(ì˜ˆ: cm, kg ë“±)ëŠ” ì“°ì§€ ë§ˆë¼.
- ê´€ì°° ê°€ëŠ¥í•œ ì‹œê°ì  íŠ¹ì§•ë§Œ ê¸°ìˆ í•˜ë¼. (í–‰ë™, ê°ì •, í’ˆì¢… ì¶”ì • ë°°ê²½ ë“±ì€ ê¸ˆì§€)
- ì¶œë ¥ì€ JSON í•˜ë‚˜ë¡œë§Œ í•´ì•¼ í•œë‹¤. í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì„ ì ˆëŒ€ ì„ì§€ ë§ˆë¼.
- ê° í•­ëª©ì˜ ê°’ì€ ë‹¨ì–´ 1~2ê°œê°€ ì•„ë‹ˆë¼, ê°€ëŠ¥í•œ í•œ ì™„ì „í•œ ë¬˜ì‚¬ ë¬¸ì¥ìœ¼ë¡œ ê¸°ìˆ í•œë‹¤.

ì¶œë ¥ ì˜ˆì‹œ:

{
"dog_or_cat_or_other": "ê°œ",
"breed_guess": "í¬ë©”ë¼ë‹ˆì•ˆ",
"body_size": "ì†Œí˜•ê²¬ìœ¼ë¡œ ì „ì²´ì ìœ¼ë¡œ ì‘ê³  ë‘¥ê·¼ ì²´í˜•",
"body_proportion": "ëª¸í†µì€ ì§§ê³  í†µí†µí•œ í˜•íƒœë¡œ, ëª©ì´ ì§§ê³  ê°€ìŠ´ì´ ë„“ìœ¼ë©° ë‹¤ë¦¬ê°€ ì§§ìŒ",
"leg_length": "ì§§ìŒ",
"fur_color_primary": "ë°ì€ í™©ê¸ˆë¹›ì— ì•½ê°„ì˜ í¬ë¦¼í†¤ì´ ì„ì¸ ë”°ëœ»í•œ ê¸ˆìƒ‰ ê³„ì—´ì˜ í„¸ìƒ‰",
"fur_color_secondary": "ì–¼êµ´ ì£¼ë³€ê³¼ ê°€ìŠ´, ê¼¬ë¦¬ ë°‘ë¶€ë¶„ì— í¬ë¯¸í•œ í°ìƒ‰ì´ ì„ì„",
"fur_pattern": "ëª¸í†µì€ ê±°ì˜ ë‹¨ìƒ‰ì— ê°€ê¹Œìš°ë©° ì–¼êµ´ê³¼ ê·€ ì£¼ë³€ë§Œ ì‚´ì§ ë°ìŒ",
"fur_length": "ê¸¸ê³  í’ì„±í•˜ë©° ëª¸ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” í˜•íƒœ",
"fur_texture": "ì•ˆìª½ì€ ì†œí„¸ì²˜ëŸ¼ ê°€ë³ì§€ë§Œ ê²‰ë¶€ë¶„ì€ ì‚´ì§ ê±°ì¹¨",
"ear_shape": "ì‘ê³  ì‚¼ê°í˜•ìœ¼ë¡œ ê·€ ëì´ ì‚´ì§ ë‘¥ê¸€ë©° ì „ì²´ì ìœ¼ë¡œ ê· í˜• ì¡íŒ í˜•íƒœ",
"ear_position": "ë¨¸ë¦¬ ìœ—ë¶€ë¶„ ì¤‘ì•™ì— ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë©°, ì„œë¡œ ì•½ê°„ ë–¨ì–´ì ¸ ìˆìŒ",
"ear_type": "ë°˜ ì¯¤ ì„œìˆëŠ” í˜•íƒœ",
"ear_tip_shape": "ëì´ ì‚´ì§ ë‘¥ê¸€ê³  ë¶€ë“œëŸ¬ìš´ í˜•íƒœ",
"eye_shape": "ë‘¥ê¸€ì§€ë§Œ ê°€ì¥ìë¦¬ë¡œ ê°ˆìˆ˜ë¡ ì‚´ì§ ì•„ëª¬ë“œí˜•ìœ¼ë¡œ ì¢ì•„ì§",
"eye_color": "ì§™ì€ ë‹¤í¬ë¸Œë¼ìš´ìœ¼ë¡œ, ë¹›ì— ë”°ë¼ ë¯¸ì„¸í•˜ê²Œ í˜¸ë°•ìƒ‰ í†¤ì´ ì„ì„",
"eye_size_ratio": "ì–¼êµ´ ëŒ€ë¹„ ëˆˆì´ í¬ê³ , ì½”ì™€ì˜ ê°„ê²©ì´ ì¢ìŒ",
"snout_length": "ì§§ê³  ë‘¥ê·¼ í˜•íƒœë¡œ, ì½” ëì´ ì‚´ì§ ìœ„ë¡œ ë“¤ë ¤ ìˆìŒ",
"snout_shape": "ì£¼ë‘¥ì´ëŠ” ì‘ê³  ë‘¥ê¸€ë©° í„¸ë¡œ ì¸í•´ ìœ¤ê³½ì´ ë¶€ë“œëŸ¬ì›€",
"nose_color": "ì§™ì€ ê²€ì •ìƒ‰ìœ¼ë¡œ ë°˜ë“¤ë°˜ë“¤í•œ ì§ˆê°",
"tail_shape": "ê¼¬ë¦¬ëŠ” ë“± ìœ„ë¡œ ë§ë ¤ ì˜¬ë¼ê°€ë©° ë¶€ì±„ê¼´ë¡œ í¼ì§„ í˜•íƒœ",
"tail_fur": "ë§¤ìš° í’ì„±í•˜ê³  ê¸¸ë©°, ê¼¬ë¦¬ ëë¶€ë¶„ì€ ë°”ê¹¥ìª½ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ë§ë ¤ ìˆìŒ",
"age_hint": "ì„±ê²¬ìœ¼ë¡œ ë³´ì„",
"unique_traits": "ê·€ ë ë¶€ë¶„ì´ ë‹¤ë¥¸ í„¸ë³´ë‹¤ ì‚´ì§ ì§™ì€ ìƒ‰ì„ ë ë©°, ì˜¤ë¥¸ìª½ ë³¼ì— í¬ë¯¸í•œ ë°ì€ìƒ‰ í„¸ ì–¼ë£©ì´ ìˆìŒ"
}

"""

# --- 2. í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
def attribute_to_text(attr_key, attr_value):
    if isinstance(attr_value, list):
        return f"{attr_key}: " + ",".join(map(str, attr_value))
    else:
        return f"{attr_key}: {attr_value}"

def extract_json_from_text(text):
    # ì½”ë“œ ë¸”ë¡ ë˜ëŠ” ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°
    text = text.strip()
    # ë°±í‹± ì œê±°
    text = re.sub(r"^```(?:json)?", "", text)
    text = re.sub(r"```$", "", text)
    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
    match = re.search(r"\{[\s\S]*\}", text)
    if match:
        return match.group(0)
    else:
        return None

# --- 3. ì´ë¯¸ì§€ ë¶„ì„ (LLM í˜¸ì¶œ) ---
def analyze_image_bytes(image_data_base64, image_name_for_log):
    """
    Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ ë°›ì•„ LLM ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (S3/ë¡œì»¬ ê³µìš©)
    """
    print(f"[LLM ë¶„ì„ì¤‘] {image_name_for_log}")
    final_prompt = prompt
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[ { "role": "user", "content": [ {"type": "text", "text": final_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data_base64}"}} ] } ],
            temperature=0
        )
        text = resp.choices[0].message.content
        json_str = extract_json_from_text(text)
        if not json_str:
            print(f"[ê²½ê³ ] JSON ê°ì§€ ì‹¤íŒ¨: {image_name_for_log}")
            return None
        return json.loads(json_str)
    except Exception as e:
        print(f"âŒ [LLM ì˜¤ë¥˜] {image_name_for_log} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def analyze_image_with_llm(image_path):
    """
    ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„, ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(image_path):
        print(f"âŒ [ì˜¤ë¥˜] ì¿¼ë¦¬ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None
    
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode("utf-8")
    
    # (í•µì‹¬) S3ìš©ìœ¼ë¡œ ë§Œë“  í•¨ìˆ˜ë¥¼ ì—¬ê¸°ì„œ ì¬ì‚¬ìš©
    return analyze_image_bytes(image_data, image_path)

# --- ì…ì–‘ìš© í”„ë¡¬í”„íŠ¸ ë° ì½”ë“œ ---
# â—€â—€ [ì‹ ê·œ ì¶”ê°€] ìì—°ì–´ -> JSON ë²ˆì—­ìš© í”„ë¡¬í”„íŠ¸
prompt_for_text_query = """
ë°˜ë“œì‹œ ì•„ë˜ JSON êµ¬ì¡° ì™¸ì˜ ì–´ë–¤ ë§ë„ í•˜ì§€ ë§ˆë¼.
ì‘ë‹µì´ JSON ì™¸ì˜ ë¬¸ìë¥¼ í¬í•¨í•˜ë©´ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬ëœë‹¤.
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬, ë™ë¬¼ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” JSON ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì—ì„œ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” **ëª¨ë“ ** íŠ¹ì§•ì„ ì•„ë˜ JSON êµ¬ì¡°ì— ë§ì¶° **ì •í™•íˆ** ì¶œë ¥í•˜ë¼.
ì‚¬ìš©ìê°€ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ì†ì„±ì€ `null`ì´ë‚˜ ë¹ˆ ë¬¸ìì—´ `""`ë¡œ ë‚¨ê²¨ë‘ì„¸ìš”.
ì ˆëŒ€ JSON ì™¸ì˜ ë‹¤ë¥¸ ë§ì„ í•˜ê±°ë‚˜ ì„¤ëª…ì„ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.

[ì‚¬ìš©ì ìš”ì²­]
{user_query}

[ì¶œë ¥ JSON í˜•ì‹]
{
"dog_or_cat_or_other": "ê°œ/ê³ ì–‘ì´/ê¸°íƒ€ ì¤‘ í•˜ë‚˜",
"breed_guess": "ì¶”ì • í’ˆì¢… (ì˜ˆ: í‘¸ë“¤, ì½”ë¦¬ì•ˆ ìˆí—¤ì–´)",
"body_size": "ì²´í˜• (ì˜ˆ: ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜•)",
"body_proportion": "ì²´í˜• ë¹„ìœ¨ (ì˜ˆ: ë‹¤ë¦¬ê°€ ì§§ìŒ, ë‚ ì”¬í•¨)",
"leg_length": "ë‹¤ë¦¬ ê¸¸ì´ (ì˜ˆ: ì§§ìŒ, ë³´í†µ, ê¹€)",
"fur_color_primary": "ì£¼ìš” í„¸ ìƒ‰ (ì˜ˆ: í°ìƒ‰, ê²€ì€ìƒ‰, ê°ˆìƒ‰, ì¹˜ì¦ˆíƒœë¹„)",
"fur_color_secondary": "ë³´ì¡° í„¸ ìƒ‰ (ì˜ˆ: ê°€ìŠ´ì— í°ìƒ‰ ë°˜ì )",
"fur_pattern": "í„¸ ë¬´ëŠ¬ (ì˜ˆ: ë‹¨ìƒ‰, ì¤„ë¬´ëŠ¬, ì ë°•ì´)",
"fur_length": "í„¸ ê¸¸ì´ (ì˜ˆ: ë‹¨ëª¨, ì¥ëª¨)",
"fur_texture": "í„¸ ì§ˆê° (ì˜ˆ: ë³µìŠ¬ë³µìŠ¬í•¨, ë¶€ë“œëŸ¬ì›€, ê±°ì¹¨)",
"ear_shape": "ê·€ ëª¨ì–‘ (ì˜ˆ: ë¾°ì¡±í•¨, ì ‘í˜)",
"ear_position": "ê·€ ìœ„ì¹˜",
"ear_type": "ê·€ íƒ€ì… (ì˜ˆ: ì«‘ê¸‹í•¨, ì¶• ëŠ˜ì–´ì§)",
"ear_tip_shape": "ê·€ ë ëª¨ì–‘",
"eye_shape": "ëˆˆ ëª¨ì–‘ (ì˜ˆ: ë‘¥ê·¼, ì•„ëª¬ë“œí˜•)",
"eye_color": "ëˆˆ ìƒ‰ (ì˜ˆ: íŒŒë€ìƒ‰, ê°ˆìƒ‰)",
"eye_size_ratio": "ì–¼êµ´ ëŒ€ë¹„ ëˆˆ í¬ê¸°",
"snout_length": "ì£¼ë‘¥ì´ ê¸¸ì´ (ì˜ˆ: ì§§ìŒ, ê¹€)",
"snout_shape": "ì£¼ë‘¥ì´ ëª¨ì–‘",
"nose_color": "ì½” ìƒ‰",
"tail_shape": "ê¼¬ë¦¬ ëª¨ì–‘",
"tail_fur": "ê¼¬ë¦¬ í„¸",
"age_hint": "ë‚˜ì´ëŒ€ (ì˜ˆ: ìƒˆë¼, ì„±ê²¬/ì„±ë¬˜, ë…¸ê²¬/ë…¸ë¬˜)",
"unique_traits": "ê¸°íƒ€ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê³ ìœ  íŠ¹ì§•"
}
"""

# â—€â—€ [2. ì‹ ê·œ ì¶”ê°€] JSON í‚¤(Key) ì²­ì†Œ í•¨ìˆ˜
def clean_json_keys(d):
    """
    LLMì´ ë°˜í™˜í•œ ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(key)ë¥¼ ê°•ì œë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
    (ì˜ˆ: '\n"key"' -> 'key')
    """
    if not isinstance(d, dict):
        return d
    
    clean_dict = {}
    for k, v in d.items():
        # 1. â—€â—€ (ìˆ˜ì •ë¨) í‚¤(key)ì—ì„œ ëª¨ë“  ê³µë°±, ì¤„ë°”ê¿ˆ, ë”°ì˜´í‘œë¥¼ 'ê°•ì œ' ì œê±°
        clean_k = k.replace('"', '').replace("'", "").replace("\n", "").strip()
        
        # 2. ê°’(value)ì´ ë”•ì…”ë„ˆë¦¬ë¼ë©´ ì¬ê·€ì ìœ¼ë¡œ ì²­ì†Œ
        clean_v = clean_json_keys(v)
        
        clean_dict[clean_k] = clean_v
    return clean_dict
    
# â—€â—€ [ìˆ˜ì •ë¨] ìì—°ì–´ -> JSON ë²ˆì—­ í•¨ìˆ˜ (í‚¤ ì²­ì†Œ ë¡œì§ ì¶”ê°€)
def analyze_text_with_llm(user_query_text):
    """
    ì‚¬ìš©ìì˜ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ë°›ì•„ LLMì„ í†µí•´ JSON ì†ì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
    ê²°ê³¼ JSONì˜ í‚¤(key)ë¥¼ ì •ë¦¬(clean)í•©ë‹ˆë‹¤.
    """
    print(f"[LLM í…ìŠ¤íŠ¸ ë¶„ì„ì¤‘] {user_query_text}")
    
    # 1. í”„ë¡¬í”„íŠ¸ ì™„ì„±
    final_prompt = prompt_for_text_query.replace("{user_query}", user_query_text)
    
    try:
        # 2. LLM í˜¸ì¶œ
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[ { "role": "user", "content": final_prompt } ],
            temperature=0
        )
        text = resp.choices[0].message.content
        
        # 3. JSON íŒŒì‹±
        json_str = extract_json_from_text(text)
        if not json_str:
            print(f"[ê²½ê³ ] JSON ê°ì§€ ì‹¤íŒ¨ (í…ìŠ¤íŠ¸ ì¿¼ë¦¬): {user_query_text}")
            return None
        
        raw_obj = json.loads(json_str)
        
        # 4. â—€â—€ [í•µì‹¬ ìˆ˜ì •] LLMì´ ë°˜í™˜í•œ JSONì˜ í‚¤(Key)ë¥¼ ê°•ì œë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        clean_obj = clean_json_keys(raw_obj)
        
        return clean_obj # â—€ ì •ë¦¬ëœ(clean) ê°ì²´ë¥¼ ë°˜í™˜
    
    except Exception as e:
        print(f"âŒ [LLM ì˜¤ë¥˜] í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ë¶„ì„/íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- 4. ì†ì„±ë³„ ì„ë² ë”© ìƒì„± ---
def get_embeddings_for_attributes(attr_dict):
    """
    JSON ê°ì²´ë¥¼ ë°›ì•„, ìœ íš¨í•œ(ë¹„ì–´ìˆì§€ ì•Šì€) ê°’ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.
    """
    attr_embeds = {} # â—€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì‹œì‘
    valid_embeddings_for_merge = [] # â—€ í‰ê· ('__merged__') ê³„ì‚°ì„ ìœ„í•œ ìœ íš¨í•œ ë²¡í„° ë¦¬ìŠ¤íŠ¸

    # â—€ (ìˆ˜ì •) .keys()ì™€ .values()ë¥¼ í•¨ê»˜ ìˆœíšŒ
    for key, value in attr_dict.items():
        
        # â—€ (í•µì‹¬) ìœ íš¨ì„± ê²€ì‚¬: ê°’ì´ Noneì´ê±°ë‚˜, ë¹ˆ ë¬¸ìì—´("")ì´ ì•„ë‹Œì§€ í™•ì¸
        text_value = str(value) # str(None)ì€ "None"ì´ ë¨
        
        if value is None or text_value.strip() == "" or text_value.strip() == "None":
            attr_embeds[key] = None # â—€ ì´ ì†ì„±ì˜ ë²¡í„°ëŠ” None (ê²€ìƒ‰ ì‹œ skipë¨)
            continue # â—€ ë‹¤ìŒ í‚¤ë¡œ ë„˜ì–´ê° (ì„ë² ë”© API í˜¸ì¶œ ì•ˆ í•¨)

        # â—€ (ì •ìƒ) ìœ íš¨í•œ ê°’ì´ë¯€ë¡œ API í˜¸ì¶œ
        try:
            emb = client.embeddings.create(model="text-embedding-3-large", input=[text_value]).data[0].embedding
            attr_embeds[key] = emb
            
            # â—€ '__merged__' ê³„ì‚°ìš© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ë‹¨, __merged__ í‚¤ ìì²´ëŠ” ì œì™¸)
            if key != "__merged__":
                valid_embeddings_for_merge.append(emb)
                
        except Exception as e:
            # (ë°©ì–´ ì½”ë“œ) ë§Œì•½ APIê°€ íŠ¹ì • ê°’(ì˜ˆ: "ì•Œìˆ˜ì—†ìŒ")ì„ ê±°ë¶€í•  ê²½ìš°
            print(f"âš ï¸ [ì„ë² ë”© ê²½ê³ ] '{key}' ê°’ '{text_value}'ì˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            attr_embeds[key] = None

    # â—€ (ìˆ˜ì •) ìœ íš¨í•œ ë²¡í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆì„ ë•Œë§Œ '__merged__' ìƒì„±
    if valid_embeddings_for_merge:
        merged_emb = np.mean(np.array(valid_embeddings_for_merge), axis=0).tolist()
        attr_embeds["__merged__"] = merged_emb
    else:
        # (ë°©ì–´ ì½”ë“œ) ìœ íš¨í•œ ê°’ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ __merged__ë„ None
        attr_embeds["__merged__"] = None

    return attr_embeds

# --- 5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ---
def cosine(a,b):
    a = np.array(a); b = np.array(b)
    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b)+1e-10))

# --- 6. ê°€ì¤‘ì¹˜ ì„¤ì • ---
weights = {

    # í’ˆì¢… ì¶”ì •
    "breed_guess": 0.250,

    # ì²´í˜• ê´€ë ¨
    "body_size": 0.090,
    "body_proportion": 0.070,
    "leg_length": 0.030,

    # í„¸ ê´€ë ¨
    "fur_color_primary": 0.800,
    "fur_color_secondary": 0.220,
    "fur_pattern": 0.100,
    "fur_length": 0.050,
    "fur_texture": 0.040,

    # ì–¼êµ´ ìš”ì†Œ
    "ear_shape": 0.080,
    "ear_position": 0.040,
    "ear_type": 0.060,
    "ear_tip_shape": 0.060,
    "eye_shape": 0.050,
    "eye_color": 0.080,
    "eye_size_ratio": 0.030,
    "snout_length": 0.060,
    "snout_shape": 0.080,
    "nose_color": 0.030,

    # ê¼¬ë¦¬ ìš”ì†Œ
    "tail_shape": 0.080,
    "tail_fur": 0.050,

    # ê°œì²´ì‹ë³„ ê°•í™”
    "unique_traits": 0.100,
    "age_hint": 0.200,

    # ë³´ì¡°ìš”ì†Œ
    "__merged__": 0.080
}

# --- 7. ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ìˆ˜ì • ---
def compare_query_to_item(query_attr_emb, item, exponent=3.0):
    score, total_w = 0.0, 0.0
    
    # 1. weights ë”•ì…”ë„ˆë¦¬ë¥¼ ìˆœíšŒ
    for k, w in weights.items():
        
        # 2. ì¿¼ë¦¬ì™€ DB ì•„ì´í…œ ì–‘ìª½ì— ëª¨ë‘ í•´ë‹¹ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        if k in query_attr_emb and k in item["attr_embeddings"]:
            
            # 3. â—€â—€ [í•µì‹¬ ìˆ˜ì •] ì¿¼ë¦¬ ë²¡í„°(vec_a) ë˜ëŠ” DB ë²¡í„°(vec_b)ê°€
            #    (get_embeddings_for_attributesì—ì„œ ìƒì„±ëœ) 'None'ì´ ì•„ë‹Œì§€ í™•ì¸
            vec_a = query_attr_emb[k]
            vec_b = item["attr_embeddings"][k]
            
            if vec_a is None or vec_b is None:
                continue # â—€ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ Noneì´ë©´, ì´ ì†ì„± ë¹„êµëŠ” ê±´ë„ˆë›´ë‹¤
            
            # 4. (ì•ˆì „) ë‘ ë²¡í„°ê°€ ëª¨ë‘ ìœ íš¨í•˜ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            sim = cosine(vec_a, vec_b)

            # 5. (ì•ˆì „) ì ìˆ˜ ë³´ì • ë° ê°€ì¤‘ì¹˜ í•©ì‚°
            calibrated_sim = ((sim + 1) / 2) ** exponent
            score += w * calibrated_sim
            total_w += w

    if total_w == 0: # (ë°©ì–´ ì½”ë“œ) ë§Œì•½ ìœ íš¨í•œ ë¹„êµê°€ í•˜ë‚˜ë„ ì—†ì—ˆë‹¤ë©´ 0 ë°˜í™˜
        return 0.0
        
    return score / (total_w + 1e-8)

def get_s3_client():
    print("NCS (S3) í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘... (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)")
    # (ìˆ˜ì •) â—€â—€ í•˜ë“œì½”ë”©ëœ í‚¤ ëŒ€ì‹  os.environì„ ì‚¬ìš©
    s3 = boto3.client('s3',
                       endpoint_url=endpoint_url,
                       region_name=region_name,
                       aws_access_key_id=os.environ.get("NCP_ACCESS_KEY"),
                       aws_secret_access_key=os.environ.get("NCP_SECRET_KEY")
                     )
    return s3

# llm_animal.pyì˜ update_db_from_s3 í•¨ìˆ˜ (ë®ì–´ì“°ê¸°)

def update_db_from_s3(s3_folder_path, db_file, id_map_file):
    s3 = get_s3_client()
    
    # 1. â—€â—€ [ìˆ˜ì •] ê¸°ì¡´ DB ë¡œë“œ -> "ë§µ(Map)"ìœ¼ë¡œ ë³€í™˜ (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•¨)
    print(f"'{db_file}'ì—ì„œ ê¸°ì¡´ DB ë¡œë“œ ì¤‘...")
    old_db_map = {}
    if os.path.exists(db_file):
        try:
            with open(db_file, "r", encoding="utf-8") as f:
                db_full_old = json.load(f)
                # íŒŒì¼ëª…ì„ keyë¡œ, item ì „ì²´ë¥¼ valueë¡œ í•˜ëŠ” ë§µ ìƒì„±
                old_db_map = {item['filename']: item for item in db_full_old}
            print(f"í˜„ì¬ DB í•­ëª©: {len(old_db_map)}ê°œ (ë§µìœ¼ë¡œ ë¡œë“œ)")
        except Exception as e:
            print(f"âš ï¸ ê²½ê³ : ê¸°ì¡´ {db_file} ë¡œë“œ/íŒŒì‹± ì‹¤íŒ¨. DBë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤. {e}")
            db_full_old = [] # (Just in case)
    else:
        db_full_old = [] # (Just in case)
        print("ê¸°ì¡´ DB íŒŒì¼ ì—†ìŒ. DBë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # 2. S3 ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì›ë³¸ ë™ì¼)
    print(f"NCS ë²„í‚· '{bucket_name}'ì˜ '{s3_folder_path}' í´ë”ì—ì„œ **í˜„ì¬** íŒŒì¼ ëª©ë¡ ì¡°íšŒ...")
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder_path)
        if 'Contents' not in response:
            print(f"âŒ [ì˜¤ë¥˜] S3 í´ë” '{s3_folder_path}'ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # (ìˆ˜ì •) â—€ S3 í´ë”ê°€ ë¹„ì–´ìˆë‹¤ë©´, ë¹ˆ DBë¥¼ ì €ì¥í•˜ê³  ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            with open(db_file, "w", encoding="utf-8") as f:
                json.dump([], f)
            with open(id_map_file, "w", encoding="utf-8") as f:
                json.dump([], f)
            print(f"âœ… S3 í´ë”ê°€ ë¹„ì–´ìˆì–´, '{db_file}'ì„(ë¥¼) ë¹ˆ íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return True # â—€ FAISS ì¬êµ¬ì¶• ì‹ í˜¸
        
        image_keys = [obj['Key'] for obj in response['Contents'] if obj['Key'].lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"S3ì—ì„œ ì´ {len(image_keys)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. (ì‚­ì œëœ íŒŒì¼ì€ ì œì™¸ë¨)")
        
    except Exception as e:
        print(f"âŒ [S3 ì˜¤ë¥˜] ìŠ¤í† ë¦¬ì§€ ì—°ê²° ë˜ëŠ” ëª©ë¡ ì¡°íšŒë¥¼ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return False

    # 3. â—€â—€ [ì‹ ê·œ] "ìƒˆë¡œìš´ DB"ë¥¼ ë‹´ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    new_db_full = []
    new_id_to_filename = []
    new_item_count = 0
    synced_item_count = 0
    
    # 4. (í•µì‹¬) "í˜„ì¬ S3 ëª©ë¡ (image_keys)"ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒˆ DBë¥¼ ì¬êµ¬ì„±
    #    (S3ì—ì„œ ì‚­ì œëœ íŒŒì¼ì€ ì´ ë£¨í”„ì— í¬í•¨ë˜ì§€ ì•ŠìŒ)
    for i, s3_key in enumerate(image_keys):
        
        # --- (A) ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ” íŒŒì¼ (ë°ì´í„° ì¬ì‚¬ìš©, ë¹„ìš© ì ˆì•½) ---
        if s3_key in old_db_map:
            print(f"  [{i+1}/{len(image_keys)}] (Sync) ê¸°ì¡´ ë°ì´í„° ì¬ì‚¬ìš©: {s3_key}")
            new_db_full.append(old_db_map[s3_key]) # â—€ ê¸°ì¡´ itemì„ ê·¸ëŒ€ë¡œ ì¶”ê°€
            new_id_to_filename.append(s3_key)
            synced_item_count += 1
        
        # --- (B) S3ì— ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼ (LLM/ì„ë² ë”© ì‹¤í–‰, ë¹„ìš© ë°œìƒ) ---
        else:
            print(f"  [{i+1}/{len(image_keys)}] (New) ì‹ ê·œ ì²˜ë¦¬: {s3_key}")
            
            try:
                # 3-1. S3 ë‹¤ìš´ë¡œë“œ
                obj = s3.get_object(Bucket=bucket_name, Key=s3_key)
                image_bytes = obj['Body'].read()
                image_data_b64 = base64.b64encode(image_bytes).decode("utf-8")
                
                # 3-2. LLM ë¶„ì„ (ë¹„ìš© ë°œìƒ ë¶€ë¶„)
                obj_attr = analyze_image_bytes(image_data_b64, s3_key)
                if obj_attr is None: continue
                
                # (user_num íŒŒì‹± ë¡œì§ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€)
                parsed_user_num = None
                if s3_folder_path in s3_key: 
                    filename_only = s3_key.split('/')[-1] 
                    match = re.match(r'^(\d+)_', filename_only) 
                    if match:
                        parsed_user_num = int(match.group(1))
                        obj_attr['user_num'] = parsed_user_num
                        print(f"    [Info] íŒŒì¼ëª…ì—ì„œ user_num: {parsed_user_num} ì¶”ì¶œ ì™„ë£Œ.")
                    else:
                        print(f"    [Warn] íŒŒì¼ëª… {filename_only}ì—ì„œ user_numì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # 3-3. ì„ë² ë”© (ë¹„ìš© ë°œìƒ ì—†ìŒ)
                emb = get_embeddings_for_attributes(obj_attr)
                if emb is None: continue
                    
                # 3-4. "ìƒˆ ë¦¬ìŠ¤íŠ¸"ì— Append
                new_item = {
                    "filename": s3_key,
                    "attributes": obj_attr,
                    "attr_embeddings": emb
                }
                new_db_full.append(new_item) # â—€ "ìƒˆ ë¦¬ìŠ¤íŠ¸"ì— ì¶”ê°€
                new_id_to_filename.append(s3_key) # â—€ "ìƒˆ ë¦¬ìŠ¤íŠ¸"ì— ì¶”ê°€
                new_item_count += 1
                
            except Exception as e:
                print(f"âŒ [ì˜¤ë¥˜] {s3_key} ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨: {e}")

    # 5. â—€â—€ [ìˆ˜ì •] ë³€ê²½ ì‚¬í•­ ê°ì§€ ë° ì €ì¥
    deleted_item_count = len(old_db_map) - synced_item_count
    
    if new_item_count == 0 and deleted_item_count == 0:
        print(f"\nâœ… DB ê°±ì‹  ì™„ë£Œ. (ì¶”ê°€: 0, ì‚­ì œ: 0). {db_file}ì„(ë¥¼) ìˆ˜ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # (ì¤‘ìš”) â—€ ë³€ê²½ì´ ì—†ì–´ë„ FAISS ì¬êµ¬ì¶•ì€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ True ë°˜í™˜
        return True 

    # 6. JSON íŒŒì¼ ì €ì¥ (ë³€ê²½ëœ ê²½ìš° "new_db_full"ë¡œ ë®ì–´ì“°ê¸°)
    print(f"\n{new_item_count}ê°œ ì¶”ê°€, {deleted_item_count}ê°œ ì‚­ì œë¨. ìƒˆ DB ì €ì¥ ì¤‘...")
    with open(db_file, "w", encoding="utf-8") as f:
        json.dump(new_db_full, f, ensure_ascii=False, indent=2)
    with open(id_map_file, "w", encoding="utf-8") as f:
        json.dump(new_id_to_filename, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… DB ì €ì¥ ì™„ë£Œ (ì´ {len(new_db_full)}ê°œ í•­ëª©)")
    return True # â—€ DB ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ FAISS ì¬êµ¬ì¶• ì‹ í˜¸

def rebuild_faiss_index(db_file, index_file, id_map_file):
    print(f"\n--- FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘ ---")
    try:
        with open(db_file, "r", encoding="utf-8") as f:
            db = json.load(f)
        with open(id_map_file, "r", encoding="utf-8") as f:
            id_to_filename_check = json.load(f)
    except Exception as e:
        print(f"âŒ {db_file} ë˜ëŠ” {id_map_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    all_vectors = []
    id_map_for_faiss = []
    
    for item in db:
        vector = item.get("attr_embeddings", {}).get("__merged__")
        if vector:
            all_vectors.append(vector)
            id_map_for_faiss.append(item.get("filename"))
            
    if id_map_for_faiss != id_to_filename_check:
        print("âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] DBì™€ IDë§µì˜ ìˆœì„œê°€ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    print(f"ì´ {len(all_vectors)}ê°œì˜ ë²¡í„°ë¡œ ì¸ë±ìŠ¤ ìƒì„±...")
    all_vectors_np = np.array(all_vectors).astype('float32')
    
    faiss.normalize_L2(all_vectors_np)
    
    # (ì¤‘ìš”) â—€â—€ ì „ì—­ ë³€ìˆ˜ VECTOR_DIMENSION ì‚¬ìš©
    index = faiss.IndexFlatIP(VECTOR_DIMENSION) 
    index.add(all_vectors_np)
    
    faiss.write_index(index, index_file)
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {index_file} (ì´ {index.ntotal}ê°œ)")

# â—€â—€ [ì‹ ê·œ ì¶”ê°€] DB ë®ì–´ì“°ê¸° ì „ìš© í•¨ìˆ˜ (app.pyì—ì„œ í˜¸ì¶œ)
def refresh_missing_data_from_db():
    """
    S3ì˜ 'abandon/missing' í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬
    ì‹¤ì¢…ë™ë¬¼ DB(missing_pets.json)ì™€ ì¸ë±ìŠ¤ë¥¼ ê°•ì œë¡œ ìµœì‹ í™”í•©ë‹ˆë‹¤.
    """
    print("ğŸ”„ [Hot Reload] ì‹¤ì¢… ë™ë¬¼ DB ë™ê¸°í™” ì‹œì‘...")

    # (ì£¼ì˜) ê²½ë¡œ ì„¤ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. app.pyê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜ ê¸°ì¤€ì…ë‹ˆë‹¤.
    s3_folder = "abandon/missing"  # S3 í´ë”ëª…
    db_file = "missing_pets.json"
    map_file = "missing_map.json"
    index_file = "missing_vectors.index"

    # 1. DB ê°±ì‹  (update_db_from_s3 í•¨ìˆ˜ ì¬ì‚¬ìš©)
    success = update_db_from_s3(s3_folder, db_file, map_file)

    # 2. ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
    if success:
        rebuild_faiss_index(db_file, index_file, map_file)
        print("âœ… [Hot Reload] íŒŒì¼ ê°±ì‹  ì™„ë£Œ.")
        return True
    else:
        print("âŒ [Hot Reload] íŒŒì¼ ê°±ì‹  ì‹¤íŒ¨.")
        return False

# â—€â—€ [ì‹ ê·œ] í¬ë¡¤ë§ ë°ì´í„°(ì…ì–‘/ë³´í˜¸ì†Œ) ì „ìš© ìë™ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def refresh_crawled_data():
    """
    í¬ë¡¤ëŸ¬ê°€ ì‹¤í–‰ëœ í›„ í˜¸ì¶œë©ë‹ˆë‹¤.
    S3ì˜ 'crawled_data' í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì…ì–‘ DBì™€ ì¸ë±ìŠ¤ë¥¼ ìµœì‹ í™”í•©ë‹ˆë‹¤.
    """
    print("ğŸ”„ [Crawler Trigger] ì…ì–‘/ë³´í˜¸ì†Œ ë™ë¬¼ AI ë°ì´í„° ê°±ì‹  ì‹œì‘...")

    # ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ë˜ ê·¸ ê²½ë¡œë“¤ì„ í•˜ë“œì½”ë”©í•©ë‹ˆë‹¤.
    s3_folder = "crawled_data"
    db_file = "dog_cat_features_attr_emb.json"
    map_file = "id_map.json"
    index_file = "animal_vectors.index"

    # 1. DB ë° ë§µ íŒŒì¼ ê°±ì‹ 
    success = update_db_from_s3(s3_folder, db_file, map_file)

    # 2. ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
    if success:
        rebuild_faiss_index(db_file, index_file, map_file)
        print("âœ… [Crawler Trigger] íŒŒì¼ ê°±ì‹  ì™„ë£Œ.")
        return True
    else:
        print("âŒ [Crawler Trigger] íŒŒì¼ ê°±ì‹  ì‹¤íŒ¨.")
        return False

# --- 6. (ìˆ˜ì •ë¨) ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    
    if len(sys.argv) < 2:
        print("âŒ [ì‹¤í–‰ ì˜¤ë¥˜] ì‹¤í–‰ ëª¨ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("   (DB ê°±ì‹  ì˜ˆì‹œ) python llm_animal.py update test-cat")
        print("   (ê²€ìƒ‰ ì˜ˆì‹œ)   python llm_animal.py search ./test_cat.jpg")
        sys.exit()

    mode = sys.argv[1] # ì‹¤í–‰ ëª¨ë“œ (update ë˜ëŠ” search)
    
    # --- (A) DB ê°±ì‹  ëª¨ë“œ ---
    if mode == 'update':
        if len(sys.argv) < 6:
            print("âŒ [ì‹¤í–‰ ì˜¤ë¥˜] 'update' ëª¨ë“œëŠ” 4ê°œì˜ ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   (ì˜ˆì‹œ) python llm_animal.py update [S3í´ë”] [DBíŒŒì¼] [ë§µíŒŒì¼] [ì¸ë±ìŠ¤íŒŒì¼]")
            print("   (ì…ì–‘) python llm_animal.py update test-cat dog_cat_features_attr_emb.json id_map.json animal_vectors.index")
            print("   (ì‹¤ì¢…) python llm_animal.py update abondon/missing missing_pets.json missing_map.json missing_vectors.index")
            sys.exit()
            
        # 1. (ìˆ˜ì •) â—€ í„°ë¯¸ë„ì—ì„œ 4ê°œì˜ ì¸ìë¥¼ ë™ì ìœ¼ë¡œ ë°›ìŒ
        s3_folder_to_scan = sys.argv[2] # ì˜ˆ: 'abondon/missing'
        db_file_arg = sys.argv[3]       # ì˜ˆ: 'missing_pets.json'
        id_map_file_arg = sys.argv[4]     # ì˜ˆ: 'missing_map.json'
        index_file_arg = sys.argv[5]      # ì˜ˆ: 'missing_vectors.index'
        
        print(f"--- [DB ê°±ì‹  ì‹œì‘]: {db_file_arg} ---")
        
        # 2. (ìˆ˜ì •) â—€ ë™ì  ì¸ìë¥¼ ì‚¬ìš©í•´ DBì— Append
        success = update_db_from_s3(s3_folder_to_scan, db_file_arg, id_map_file_arg)
        
        # 3. (ìˆ˜ì •) â—€ ë™ì  ì¸ìë¥¼ ì‚¬ìš©í•´ FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        if success:
            rebuild_faiss_index(db_file_arg, index_file_arg, id_map_file_arg)
        
        print(f"--- [DB ê°±ì‹  ì™„ë£Œ]: {db_file_arg} ---")
            
    # --- (B) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ ---
    elif mode == 'search':
        if len(sys.argv) < 3:
            print("âŒ [ì‹¤í–‰ ì˜¤ë¥˜] 'search' ëª¨ë“œëŠ” ì¿¼ë¦¬ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit()
            
        query_filename = sys.argv[2]
        
        print(f"'{INDEX_FILE}'ì—ì„œ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            index = faiss.read_index(INDEX_FILE)
        except Exception as e:
            print(f"âŒ [ì˜¤ë¥˜] FAISS ì¸ë±ìŠ¤({INDEX_FILE}) ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit()
        
        print(f"'{MAP_FILE}'ì—ì„œ ID-íŒŒì¼ëª… ë§µì„ ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            with open(MAP_FILE, "r", encoding="utf-8") as f:
                id_to_filename = json.load(f)
        except Exception as e:
            print(f"âŒ [ì˜¤ë¥˜] ID ë§µ({MAP_FILE}) ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit()
            
        print(f"'{DB_FILE}'ì—ì„œ 2ë‹¨ê³„ ì¬ì •ë ¬ì„ ìœ„í•œ ì›ë³¸ DB ë¡œë“œ ì¤‘...")
        try:
            with open(DB_FILE,"r",encoding="utf-8") as f:
                db_full = json.load(f)
            print(f"âœ… ì›ë³¸ DB ë¡œë“œ ì™„ë£Œ ({len(db_full)}ê°œ í•­ëª©)")
        except Exception as e:
            print(f"âŒ ì›ë³¸ DB({DB_FILE}) ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit()

        print(f"\n[ì—…ë¡œë“œ ì™„ë£Œ] '{query_filename}' íŒŒì¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        (start_time_total) = time.time()
        
        query_obj = analyze_image_with_llm(query_filename)
        if query_obj:
            query_attr_emb = get_embeddings_for_attributes(query_obj)
            if query_attr_emb and "__merged__" in query_attr_emb:
                print(f"âœ… ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time_total:.2f}ì´ˆ)")
                
                print(f"\n--- [1ë‹¨ê³„: FAISS ì˜ˆì„ ] ì‹œì‘ (í›„ë³´ {K_CANDIDATES}ê°œ íƒìƒ‰) ---")
                (start_time_faiss) = time.time()
                query_merged_vector = query_attr_emb["__merged__"]
                query_vector_np = np.array([query_merged_vector]).astype('float32')
                faiss.normalize_L2(query_vector_np)
                D_faiss, I_faiss = index.search(query_vector_np, K_CANDIDATES)
                candidate_indices = I_faiss[0]
                print(f"âœ… FAISS ì˜ˆì„  ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time_faiss:.2f}ì´ˆ)")
                
                print(f"\n--- [2ë‹¨ê³„: ì›ë³¸ ë¡œì§ ë³¸ì„ ] ì‹œì‘ (í›„ë³´ {len(candidate_indices)}ê°œ ì¬ì •ë ¬) ---")
                (start_time_rerank) = time.time()
                query_species = query_obj.get("dog_or_cat_or_other")
                final_results = []
                for idx in candidate_indices:
                    item = db_full[idx]
                    if item.get("attributes", {}).get("dog_or_cat_or_other") == query_species:
                        score = compare_query_to_item(query_attr_emb, item)
                        final_results.append((item["filename"], score))
                final_results.sort(key=lambda x: x[1], reverse=True)
                print(f"âœ… ì›ë³¸ ë¡œì§ ë³¸ì„  ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time_rerank:.2f}ì´ˆ)")
                
                print("\n" + "="*40)
                print(f"ğŸš€ [ìµœì¢… ë§¤ì¹­ ê²°ê³¼ (Top {K_FINAL})] ğŸš€")
                print(f"(ì´ ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time_total:.2f}ì´ˆ)")
                print(f"ì¿¼ë¦¬ ì´ë¯¸ì§€: {query_filename}")
                print("="*40)
                for i, (filename, score) in enumerate(final_results[:K_FINAL]):
                    print(f"  {i+1}ìˆœìœ„: {filename} (ìœ ì‚¬ë„: {score:.4f})")
            else:
                print("âŒ [ì‹¤í–‰ ì¤‘ë‹¨] ì¿¼ë¦¬ ì´ë¯¸ì§€ì˜ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ [ì‹¤í–‰ ì¤‘ë‹¨] ì¿¼ë¦¬ ì´ë¯¸ì§€ì˜ LLM ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    else:
        print(f"âŒ [ì‹¤í–‰ ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œì…ë‹ˆë‹¤: '{mode}'")
        print("   (ì‚¬ìš© ê°€ëŠ¥ ëª¨ë“œ: 'update' ë˜ëŠ” 'search')")
