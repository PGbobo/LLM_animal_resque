import pymysql.cursors
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import csv
from datetime import datetime
from datetime import date 
import re
import sys
import math
import boto3
import os
from io import BytesIO
import llm_animal
import faiss
import numpy as np
import json
import base64

try:
    with open('./API-Key.txt','r') as f:
        os.environ['OPENAI_API_KEY'] = f.read().strip()
    with open('./ACCESS_KEY.txt','r') as f:
        os.environ['NCP_ACCESS_KEY'] = f.read().strip()
    with open('./SECRET_KEY.txt','r') as f:
        os.environ['NCP_SECRET_KEY'] = f.read().strip()
except Exception as e:
    print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] í‚¤ íŒŒì¼(API-Key.txt, ACCESS_KEY.txt, SECRET_KEY.txt) ë¡œë“œ ì‹¤íŒ¨: {e}")
    sys.exit()

# ====================================================================
# 1. í™˜ê²½ ì„¤ì • 
# ====================================================================
DB_CONFIG = {
    "host": "project-db-campus.smhrd.com",
    "port": 3307, 
    "user": "campus_24IS_CLOUD3_p3_1",
    "password": "smhrd1", 
    "database": "campus_24IS_CLOUD3_p3_1",
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

CRAWL_URL = "https://www.kcanimal.or.kr/board_gallery01/board_list.asp"
BASE_DOMAIN = "https://www.kcanimal.or.kr" 

DB_TABLE_NAME = "ANIMALS"

# ğŸ’¡ DB í…Œì´ë¸” ì»¬ëŸ¼ ëª©ë¡ (ì´ 14ê°œ ì»¬ëŸ¼: BOARD_IDX í¬í•¨)
ANIMAL_COLUMNS = ["BOARD_IDX", "NAME", "SPECIES", "BREED", "GENDER", "FEATURE", "PHOTO1", "PHOTO2", "PHOTO3", 
                 "RESCUE_DATE", "RESCUE_LOCATION", "AGE", "CRAWL_URL", "LAST_CRAWLED_AT"]

# ğŸ’¡ UPSERTë¥¼ ìœ„í•œ ê³ ìœ  í‚¤ (BOARD_IDXë§Œ ì‚¬ìš©)
UNIQUE_KEY_COLUMNS = ["BOARD_IDX"]
UNIQUE_KEY_NAME = "unique_animal_record_v9_idx" # í‚¤ ì´ë¦„ ë³€ê²½
ITEMS_PER_PAGE = 12 

try:
    NCP_CONFIG = {
        "endpoint_url": "https://kr.object.ncloudstorage.com",
        "region_name": "kr-standard",
        "aws_access_key_id": os.environ['NCP_ACCESS_KEY'], # (app.pyì™€ ë™ì¼í•œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        "aws_secret_access_key": os.environ['NCP_SECRET_KEY']
    }
    S3_BUCKET_NAME = "animal-bucket" # (app.pyì™€ ë™ì¼í•œ ë²„í‚·)
    S3_CRAWL_DIR = "crawled_data" # â—€ S3ì— ì €ì¥í•  ê¸°ë³¸ í´ë”ëª…
    
    # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    s3_client = boto3.client('s3', **NCP_CONFIG)
    print("âœ… NCP (S3) í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ.")

except KeyError:
    print("âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] NCP í™˜ê²½ë³€ìˆ˜(NCP_ACCESS_KEY, NCP_SECRET_KEY)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit()
except Exception as e:
    print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] NCP (S3) í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    sys.exit()

print("--- [Trigger 1] ì•Œë¦¼ ì„œë¹„ìŠ¤ë¥¼ ìœ„í•´ 'ì‹¤ì¢…ë™ë¬¼ DB' ë¡œë“œ ì‹œì‘ ---")
g_missing_index = None
g_missing_db_full = None
try:
    MISSING_INDEX_FILE = "missing_vectors.index"
    MISSING_MAP_FILE = "missing_map.json"
    MISSING_DB_FILE = "missing_pets.json"
    
    print(f"'{MISSING_INDEX_FILE}' (ì‹¤ì¢…DB) ë¡œë“œ ì¤‘...")
    g_missing_index = faiss.read_index(MISSING_INDEX_FILE)
    # (ë§µ íŒŒì¼ì€ í¬ë¡¤ëŸ¬ì—ì„œëŠ” í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë¡œë“œ ì•ˆ í•¨)
    print(f"'{MISSING_DB_FILE}' (ì‹¤ì¢…DB ì›ë³¸) ë¡œë“œ ì¤‘...")
    with open(MISSING_DB_FILE,"r",encoding="utf-8") as f:
        g_missing_db_full = json.load(f)
    print(f"âœ… [Trigger 1] ì‹¤ì¢…DB ë¡œë“œ ì™„ë£Œ (ì´ {len(g_missing_db_full)}ê°œ í•­ëª©)")
except Exception as e:
    print(f"âš ï¸ [Trigger 1] ì‹¤ì¢…DB íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. ì•Œë¦¼ ì„œë¹„ìŠ¤(Trigger 1)ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤: {e}")
    # (ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ê³„ì†ë˜ì–´ì•¼ í•˜ë¯€ë¡œ sys.exit() ì•ˆ í•¨)

# ====================================================================
# 2. ë°ì´í„° íŒŒì‹± ë„ìš°ë¯¸ í•¨ìˆ˜ 
# ====================================================================

def create_notification_signal(user_num, message, noti_type="IMMEDIATE"):

    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        sql = "INSERT INTO NOTIFICATIONS (user_num, message, status, type) VALUES (%s, %s, 'pending', %s)"
        curs.execute(sql, (user_num, message, noti_type))
        conn.commit()
        print(f"  [ğŸ”” ì•Œë¦¼ ì‹ í˜¸ ìƒì„± (Trigger 1)] User {user_num}ì—ê²Œ '{message[:20]}...' ì „ì†¡ ì˜ˆì•½")
        print(f"  [ğŸ”” DB ì €ì¥] User {user_num}ì—ê²Œ '{noti_type}' ì•Œë¦¼ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"  [âŒ ì•Œë¦¼ ì‹ í˜¸ ì‹¤íŒ¨ (Trigger 1)] User {user_num} DB INSERT ì‹¤íŒ¨: {e}")
        if conn: conn.rollback()
    finally:
        if curs: curs.close()
        if conn: conn.close()

def parse_date(date_str):
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
    if match:
        date_str = match.group(1)
    try:
        return datetime.strptime(date_str, '%Y-%m-%d').date() 
    except ValueError:
        return date.today()
        
def parse_age(age_str):
    """
    ë‚˜ì´ ë¬¸ìì—´ì„ ë¶„ì„í•˜ì§€ ì•Šê³ , ì•ë’¤ ê³µë°±ë§Œ ì œê±°í•œ í›„ ì›ë³¸ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    cleaned_age_str = age_str.strip()
    
    if not cleaned_age_str:
        return 'ë¯¸ìƒ'
        
    return cleaned_age_str

# â—€â—€ [ì‹ ê·œ ì¶”ê°€] S3 ì—…ë¡œë“œ í—¬í¼ í•¨ìˆ˜
def upload_image_to_s3(image_url, board_idx, image_index):
    """
    ì›ë³¸ ì´ë¯¸ì§€ URLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ S3ì— ì—…ë¡œë“œí•˜ê³ , S3 Key(ê²½ë¡œ)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not image_url:
        return None
        
    try:
        # 1. ì›ë³¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        img_response = requests.get(image_url, timeout=10)
        img_response.raise_for_status()
        img_data = BytesIO(img_response.content) # â—€ ë©”ëª¨ë¦¬ì— ì´ë¯¸ì§€ ì €ì¥
        
        # 2. S3 í‚¤ ìƒì„± (ì˜ˆ: crawled_data/38576/image_1.jpg)
        # (íŒŒì¼ í™•ì¥ìë¥¼ ì›ë³¸ URLì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜, .jpgë¡œ ê³ ì •)
        file_ext = os.path.splitext(image_url.split('?')[0])[-1] or '.jpg'
        s3_key = f"{S3_CRAWL_DIR}/{board_idx}/image_{image_index}{file_ext}"
        
        # 3. S3ì— ì—…ë¡œë“œ (ACL='public-read'ë¡œ ì„¤ì •í•´ì•¼ <img> íƒœê·¸ì—ì„œ ë³´ì„)
        s3_client.upload_fileobj(
            img_data,
            S3_BUCKET_NAME,
            s3_key,
            ExtraArgs={'ACL': 'public-read'} # â—€ (ì¤‘ìš”) ì´ë¯¸ì§€ë¥¼ ê³µê°œë¡œ ì„¤ì •
        )
        
        # 4. (ì¤‘ìš”) DBì— ì €ì¥í•  ìµœì¢… URLì´ ì•„ë‹Œ, "S3 Key"ë§Œ ë°˜í™˜
        # (Reactì—ì„œëŠ” S3_BUCKET_BASE_URL + s3_keyë¡œ ì¡°í•©í•´ì„œ ì‚¬ìš©)
        return s3_key 
        
    except requests.exceptions.RequestException:
        print(f" Â [Fail] S3 ì—…ë¡œë“œ ì‹¤íŒ¨ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜): {image_url}")
        return None
    except Exception as e:
        print(f" Â [Fail] S3 ì—…ë¡œë“œ ì‹¤íŒ¨ (Boto3 ì˜¤ë¥˜): {e}")
        return None

# ====================================================================
# 3. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ (ì‚¬ì§„ URL ì¶”ì¶œ ë¡œì§ ì¬ê°•í™”)
# ====================================================================
def fetch_detail_info(board_idx):
    if not board_idx or not str(board_idx).isdigit():
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "board_idx ì˜¤ë¥˜", None, None, None
        
    detail_url = f"{BASE_DOMAIN}/board_gallery01/board_content.asp?board_idx={board_idx}&tname=board_gallery01"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(detail_url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        # ì¶•ì¢…/í’ˆì¢… ì¶”ì¶œ
        species_th = soup.find("th", string="ì¶•ì¢…")
        species = species_th.find_next_sibling('td').text.strip() if species_th else "ë¯¸ìƒ"
        
        breed_th = soup.find("th", string="í’ˆì¢…")
        breed = breed_th.find_next_sibling('td').text.strip() if breed_th else "ë¯¸ìƒ"
        if breed == "-": breed = "ë¯¸ìƒ" 

        # ğŸ’¡ ì‚¬ì§„ URL 3ê°œ ì¶”ì¶œ ë¡œì§ ğŸ’¡
        photo_urls = []
        selectors = [
            '.board_content_img img', 
            'td[colspan="4"] img',
            'div.board_content_img_box img' 
        ]

        for selector in selectors:
            for img in soup.select(selector):
                img_src = img.get('src')
                if not img_src or 'no_img' in img_src or '.gif' in img_src:
                    continue
                
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if img_src.startswith('/'):
                    full_url = BASE_DOMAIN + img_src
                elif not img_src.startswith('http'):
                    full_url = f"{BASE_DOMAIN}/board_gallery01/" + img_src
                else:
                    full_url = img_src
                    
                if full_url not in photo_urls:
                    photo_urls.append(full_url)
                if len(photo_urls) >= 3: break
            if len(photo_urls) >= 3: break

        # ì¶”ì¶œëœ ì›ë³¸ URLì„ S3ì— ì—…ë¡œë“œí•˜ê³ , S3 Keyë¡œ êµì²´
        s3_key_1 = upload_image_to_s3(photo_urls[0] if len(photo_urls) > 0 else None, board_idx, 1)
        s3_key_2 = upload_image_to_s3(photo_urls[1] if len(photo_urls) > 1 else None, board_idx, 2)
        s3_key_3 = upload_image_to_s3(photo_urls[2] if len(photo_urls) > 2 else None, board_idx, 3)

        # íŠ¹ì§• ë° íŠ¹ì´ì‚¬í•­ ì¶”ì¶œ -> Featureë¡œ í†µí•©
        features = []
            
        feature_th = soup.find("th", text="íŠ¹ì§•")
        feature_text = feature_th.find_next_sibling('td').text.strip() if feature_th else ""
        if feature_text: features.append(f"íŠ¹ì§•:{feature_text}")
            
        special_th = soup.find("th", text="íŠ¹ì´ì‚¬í•­")
        if special_th:
            special_text = special_th.find_next_sibling('td').text.strip()
            if special_text: features.append(f"íŠ¹ì´ì‚¬í•­:{special_text}")
                
        final_feature_detail = ", ".join(features)
        
        return species, breed, final_feature_detail, s3_key_1, s3_key_2, s3_key_3

    except requests.exceptions.RequestException:
        print(f" Â [Fail] ìƒì„¸ ìš”ì²­ ì‹¤íŒ¨: {detail_url} (ë„¤íŠ¸ì›Œí¬/ì„œë²„ ì˜¤ë¥˜)")
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "ìƒì„¸ ìš”ì²­ ì˜¤ë¥˜", None, None, None 
    except Exception as e:
        print(f" Â [Fail] íŒŒì‹± ì‹¤íŒ¨: {detail_url} ({e})")
        return "ë¯¸ìƒ", "ë¯¸ìƒ", "ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜", None, None, None

# ====================================================================
# 4. ë™ì  í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ ë° ëª©ë¡ í¬ë¡¤ë§ í•¨ìˆ˜ 
# ====================================================================
def get_total_pages(url):
    """ì´ ê²Œì‹œë¬¼ ìˆ˜ë¥¼ íŒŒì‹±í•˜ì—¬ ì´ í˜ì´ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (íŒŒì‹± ì•ˆì •ì„± ê°•í™”)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')
        
        # ì´ ê²Œì‹œë¬¼ ìˆ˜ ì„ íƒì ê°•í™”
        total_count_element = soup.select_one("td.list_total strong")
        total_items = 0 
        
        if total_count_element:
            match = re.search(r'(\d+)', total_count_element.text)
            if match:
                total_items = int(match.group(1))
        
        if total_items == 0:
            text_element = soup.find(text=re.compile(r'\d+\s*ê±´'))
            if text_element:
                match = re.search(r'(\d+)', text_element)
                if match:
                    total_items = int(match.group(1))
            
        if total_items == 0:
            print("[Warning] ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 1í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
            return 1 
        else:
            total_pages = math.ceil(total_items / ITEMS_PER_PAGE)
        
        print(f" Â  Â [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì´ ê²Œì‹œë¬¼ ìˆ˜: {total_items}ê°œ, í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜: {ITEMS_PER_PAGE}ê°œ")
        print(f" Â  Â [DEBUG] ê³„ì‚°ëœ ì´ í˜ì´ì§€ ìˆ˜: {total_pages}ê°œ")
        
        return total_pages

    except requests.exceptions.RequestException:
        print(f" Â [Error] ì´ í˜ì´ì§€ ìˆ˜ ìš”ì²­ ì˜¤ë¥˜. 1í˜ì´ì§€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f" Â [Error] í˜ì´ì§€ ìˆ˜ íŒŒì‹± ì˜¤ë¥˜: {e}. 1í˜ì´ì§€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        return 1


def fetch_data(url):
    """ì§€ì •ëœ URLì—ì„œ ë™ë¬¼ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  ìƒì„¸ í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. 
    ë°˜í™˜ ê°’ì— board_idxì™€ ìƒì„¸ í˜ì´ì§€ URLì„ í¬í•¨í•©ë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'euc-kr' 
        response.raise_for_status()
        soup = bs(response.text, 'html.parser')

        # ëª©ë¡ í•­ëª© ì„ íƒì ëŒ€í­ ê°•í™” 
        items = soup.select("ul.list_gallery_ul > li, #goodsBox > ul > li, .board_list_gallery > ul > li") 
            
        print(f" Â  Â [DEBUG] URL: {url} | ë°œê²¬ëœ í•­ëª© ìˆ˜: {len(items)}ê°œ")
        
        data = []
        
        current_page_url = url # ëª©ë¡ í˜ì´ì§€ URL (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            def process_item(item):
                board_idx = "N/A"
                try:
                    link = item.select_one('a')
                    if not link or not link.get('href'): return None
                    
                    # ìƒì„¸ URLì—ì„œ board_idx ì¶”ì¶œ
                    detail_href = link['href']
                    match_idx = re.search(r'board_idx=(\d+)', detail_href)
                    if not match_idx: return None 
                    board_idx = match_idx.group(1) 
                    
                    # CRAWL_URLì— ì €ì¥í•  ìƒì„¸ URLì„ ìƒì„±
                    detail_url_to_save = f"{BASE_DOMAIN}/board_gallery01/board_content.asp?board_idx={board_idx}&tname=board_gallery01"
                    
                    # ëª©ë¡ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    p_text_el = item.select_one("div p")
                    if not p_text_el: return None
                    p_text = p_text_el.text.strip()
                    
                    match_name = re.match(r'(.+?)\s*\(\d{2}-\d+\)', p_text)
                    # ğŸ’¡ [ìˆ˜ì •ëœ ë¶€ë¶„] ì´ë¦„ì´ ì—†ìœ¼ë©´ "(ì´ë¦„ì—†ìŒ)"ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•­ëª©ì„ ë²„ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    name = match_name.group(1).strip() if match_name and match_name.group(1).strip() else "(ì´ë¦„ì—†ìŒ)"
                    
                    # ğŸ’¡ ì´ë¦„ì´ "(ì´ë¦„ì—†ìŒ)"ì¸ ê²½ìš°ì—ë„ í•­ëª©ì„ ë²„ë¦¬ì§€ ì•Šë„ë¡ í•´ë‹¹ `if` êµ¬ë¬¸ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.

                    feature_status = p_text.split(')')[-1].strip()
                    
                    span_text_el = item.select_one("div span")
                    if not span_text_el: return None
                    span_text = span_text_el.text.strip().split('|')

                    rescue_loc = span_text[0].strip() if len(span_text) > 0 else "ë¯¸ìƒ"
                    rescue_date_str = span_text[1].strip() if len(span_text) > 1 else "ë¯¸ìƒ"
                    age_str = span_text[2].strip() if len(span_text) > 2 else "0ì‚´"
                    gender = span_text[3].strip() if len(span_text) > 3 else "ë¯¸ìƒ"
                    weight = span_text[4].strip() if len(span_text) > 4 else "0kg"
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    species, breed, feature_detail, photo1, photo2, photo3 = fetch_detail_info(board_idx) 
                    
                    # í•„ìˆ˜ ë°ì´í„° ìœ íš¨ì„± ì¬í™•ì¸ (ì´ë¦„ ì œì™¸)
                    # ì¶•ì¢…, êµ¬ì¡°ì¼, í’ˆì¢… ì •ë³´ëŠ” ë°˜ë“œì‹œ ìˆì–´ì•¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
                    if species == "ë¯¸ìƒ" or rescue_date_str == "ë¯¸ìƒ" or breed == "ë¯¸ìƒ":
                        return None 

                    rescue_date = parse_date(rescue_date_str)
                    age = parse_age(age_str)
                    feature = f"ìƒíƒœ:{feature_status}, ë¬´ê²Œ:{weight}, ìƒì„¸íŠ¹ì§•:[{feature_detail}]"
                    
                    # ìµœì¢… ë°ì´í„° ë¦¬í„´ (ì´ë¦„ì´ ì—†ì–´ë„ ì €ì¥ë¨)
                    return (board_idx, name, species, breed, gender, feature, photo1, photo2, photo3, 
                            rescue_date, rescue_loc, age, detail_url_to_save)
                            
                except Exception as item_e:
                    # print(f" Â  Â [Fail] ëª©ë¡ í•­ëª© íŒŒì‹± ì‹¤íŒ¨ (idx:{board_idx}): {item_e}")
                    return None

            results = executor.map(process_item, items)
            data = [result for result in results if result is not None]

        return data
        
    except requests.exceptions.RequestException as e:
        print(f" Â [Error] ì›¹ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f" Â [Error] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        return []

# ====================================================================
# 5. DB ë° ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜ 
# ====================================================================

def initialize_db_schema():
    """DBì— ì—°ê²°í•˜ì—¬ UPSERTë¥¼ ìœ„í•œ UNIQUE KEYê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
    print("ğŸ› ï¸ DB ì´ˆê¸°í™” (UNIQUE KEY ì„¤ì •)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        key_columns_str = ', '.join(f'`{c}`' for c in UNIQUE_KEY_COLUMNS)
        sql_add_unique_key = f"""
        ALTER TABLE {DB_TABLE_NAME}
        ADD UNIQUE KEY {UNIQUE_KEY_NAME} ({key_columns_str});
        """
        
        # ê¸°ì¡´ UNIQUE KEY ì‚­ì œ ì‹œë„ (ì•ˆì •ì„± ê°•í™”)
        try:
            print(" Â [DEBUG] ê¸°ì¡´ UNIQUE KEY ì‚­ì œ ì‹œë„...")
            # ì´ì „ ë²„ì „ì˜ í‚¤ ì‚­ì œ ì‹œë„
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_num_id;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_no_breed;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_v6;")
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY unique_animal_record_test;")
            # í˜„ì¬ í‚¤ë„ í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ëŒ€ë¹„ ì‚­ì œ ì‹œë„
            curs.execute(f"ALTER TABLE {DB_TABLE_NAME} DROP KEY {UNIQUE_KEY_NAME};") 
            conn.commit()
            print(" Â [DEBUG] ì´ì „ UNIQUE KEY ì‚­ì œ ì™„ë£Œ.")
        except pymysql.err.ProgrammingError as e:
             if e.args[0] != 1091: # 1091: KEYê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
                 print(f" Â [DEBUG] ì´ì „ KEY ì‚­ì œ ì‹¤íŒ¨: {e}")
             pass 
        except Exception:
             pass 

        # ìƒˆë¡œìš´ UNIQUE KEY ì„¤ì •
        curs.execute(sql_add_unique_key)
        conn.commit()
        print(f"âœ… UNIQUE KEY '{UNIQUE_KEY_NAME}' ì„¤ì • ì™„ë£Œ: ({key_columns_str})")

    except pymysql.err.ProgrammingError as e:
        if e.args[0] == 1061: 
            print(f"âš ï¸ UNIQUE KEY '{UNIQUE_KEY_NAME}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. (ì´ˆê¸°í™” ê±´ë„ˆë›°ê¸°)")
        elif e.args[0] == 1146:
            print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: í…Œì´ë¸” '{DB_TABLE_NAME}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í…Œì´ë¸” ìƒì„± í•„ìš”)")
        elif e.args[0] == 1072: # Key column 'BOARD_IDX' doesn't exist in table
             print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: í…Œì´ë¸”ì— BOARD_IDX ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì„¸ìš”! ({e})")
        else:
            print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"âŒ DB ì—°ê²°/ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if curs: curs.close()
        if conn: conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ.")


def job_crawl_and_save():
    job_timestamp = datetime.now() 
    current_time = job_timestamp.strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n=======================================================")
    print(f"ğŸš€ ìŠ¤ì¼€ì¤„ë§ëœ ë™ë¬¼ ë°ì´í„° í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: {current_time}")
    print(f"=======================================================")

    total_pages = get_total_pages(CRAWL_URL)

    if total_pages == 0:
        print("[INFO] ì´ í˜ì´ì§€ ìˆ˜ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    urls = []
    # ğŸ’¡ í˜ì´ì§€ ì œí•œì„ total_pagesë¡œ ë³€ê²½í•˜ì—¬ ì „ì²´ í¬ë¡¤ë§ 
    max_pages_to_crawl = total_pages
    
    for page in range(1, max_pages_to_crawl + 1): 
        if page == 1:
            urls.append(CRAWL_URL) 
        else:
            urls.append(f"{CRAWL_URL}?page={page}")
            
    print(f"[INFO] í¬ë¡¤ë§í•  ìµœì¢… URL ìˆ˜: {len(urls)}ê°œ (ì´ {total_pages}í˜ì´ì§€ ëª¨ë‘ ì‹œë„)")
    
    all_data = []
    
    global g_missing_index, g_missing_db_full, s3_client
    
    # (ìˆ˜ì •) â—€ "ì‹¤ì¢…DB"ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€(ì•Œë¦¼ ê¸°ëŠ¥ í™œì„±í™”) í™•ì¸
    trigger1_enabled = g_missing_index is not None and g_missing_db_full is not None
    if trigger1_enabled:
        print(f"âœ… [Trigger 1] í™œì„±í™”ë¨. í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ 'ì‹¤ì¢…DB'ì™€ ë¹„êµí•©ë‹ˆë‹¤.")
    else:
        print(f"âš ï¸ [Trigger 1] ë¹„í™œì„±í™”ë¨. 'ì‹¤ì¢…DB' ë¡œë“œì— ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ì•Œë¦¼ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
    alerted_owners_for_board = {} # â—€ (ì‹ ê·œ) ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ìš© (board_idx: {user_num, user_num})

    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_data, urls)
        for result_list_per_page in results:
            
            # 1. (ì›ë³¸) â—€ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ all_data ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            all_data.extend(result_list_per_page) 
            
            # 2. (ì‹ ê·œ) â—€ Trigger 1 ë¡œì§ (ì‹¤ì¢…DBê°€ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰)
            if trigger1_enabled and result_list_per_page:
                
                print(f"  [Trigger 1] {len(result_list_per_page)}ê°œ ì‹ ê·œ ë°ì´í„° AI ë¹„êµ ì‹œì‘...")
                
                # ê°“ í¬ë¡¤ë§ëœ í˜ì´ì§€ì˜ ë™ë¬¼ë“¤(result_list_per_page)ì„ í•˜ë‚˜ì”© ìˆœíšŒ
                for new_animal_tuple in result_list_per_page:
                    try:
                        # (ì°¸ê³ : ANIMAL_COLUMNS ìˆœì„œì™€ ë™ì¼í•¨)
                        # (board_idx, name, species, breed, gender, feature, 
                        #  photo1, photo2, photo3, rescue_date, ...)
                        board_idx = new_animal_tuple[0]
                        photo1_s3_key = new_animal_tuple[6] # â—€ 7ë²ˆì§¸ ê°’ (PHOTO1 S3 Key)
                        
                        if not photo1_s3_key:
                            continue # â—€ ì‚¬ì§„ ì—†ìœ¼ë©´ ë¹„êµ ë¶ˆê°€
                            
                        # 2-1. (ëŠë¦° ì‘ì—…) â—€ S3ì—ì„œ ë°©ê¸ˆ ì˜¬ë¦° ì‚¬ì§„ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
                        obj = s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=photo1_s3_key)
                        image_bytes = obj['Body'].read()
                        image_data_b64 = base64.b64encode(image_bytes).decode("utf-8")

                        # 2-2. (ëŠë¦° ì‘ì—…) â—€ LLM ë¶„ì„ìœ¼ë¡œ ë²¡í„° ìƒì„±
                        query_obj = llm_animal.analyze_image_bytes(image_data_b64, f"crawl_{board_idx}.jpg")
                        if not query_obj: continue
                        query_attr_emb = llm_animal.get_embeddings_for_attributes(query_obj)
                        if not (query_attr_emb and "__merged__" in query_attr_emb):
                            continue
                            
                        # 2-3. (ë¹ ë¥¸ ì‘ì—…) â—€ "ì‹¤ì¢… DB" ê²€ìƒ‰
                        query_merged_vector = query_attr_emb["__merged__"]
                        query_vector_np = np.array([query_merged_vector]).astype('float32')
                        faiss.normalize_L2(query_vector_np)
                        
                        D_faiss, I_faiss = g_missing_index.search(query_vector_np, llm_animal.K_CANDIDATES)
                        candidate_indices = I_faiss[0]
                        
                        query_species = query_obj.get("dog_or_cat_or_other")
                        
                        # 2-4. 80% ì´ìƒ ë§¤ì¹­ í™•ì¸
                        for idx in candidate_indices:
                            missing_item = g_missing_db_full[idx]
                            
                            if missing_item.get("attributes", {}).get("dog_or_cat_or_other") == query_species:
                                score = llm_animal.compare_query_to_item(query_attr_emb, missing_item)
                                
                                if score >= 0.80:
                                    owner_user_num = missing_item.get("attributes", {}).get("user_num")
                                    if not owner_user_num: continue
                                        
                                    # â—€ ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€
                                    if board_idx not in alerted_owners_for_board:
                                        alerted_owners_for_board[board_idx] = set()

                                    if owner_user_num not in alerted_owners_for_board[board_idx]:
                                        # 1. íŒŒì¼ëª…ì—ì„œ 'ì´ë¦„'ë§Œ ì˜ˆì˜ê²Œ ì¶”ì¶œí•˜ê¸°
                                        full_path = missing_item.get('filename', '') # ì˜ˆ: abandon/missing/15_ì²œì‚¬_1764...jpg
                                        pet_name = "ë°˜ë ¤ë™ë¬¼" # ê¸°ë³¸ê°’
                                        try:
                                            # ê²½ë¡œ ë–¼ê³  íŒŒì¼ëª…ë§Œ (15_ì²œì‚¬_1764...jpg)
                                            file_only = full_path.split('/')[-1]
                                            # ì–¸ë”ë°”(_)ë¡œ ìª¼ê°œì„œ ë‘ ë²ˆì§¸ ë©ì–´ë¦¬(ì´ë¦„) ê°€ì ¸ì˜¤ê¸°
                                            pet_name = file_only.split('_')[1]
                                        except:
                                            pass # ì´ë¦„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©

                                        print(f"  [ğŸ”” 80% ë§¤ì¹­ (Trigger 1)] ì‹ ê·œ(idx:{board_idx}) â†” ì‹¤ì¢…({pet_name})")
                                        
                                        # 2. ë©”ì‹œì§€ í¬ë§·ì„ 'ì œë³´' ë•Œì™€ ë˜‘ê°™ì´ ë§ì¶¤ (ì˜¤íƒ€ ìˆ˜ì • í¬í•¨)
                                        message = f"[ì´ì–´ì£¼ê°œ] íšŒì›ë‹˜ì˜ ì‹¤ì¢…ë™ë¬¼'{pet_name}'ê³¼(ì™€) {score*100:.0f}% ìœ ì‚¬í•œ ë™ë¬¼ì´ ê´‘ì£¼ê´‘ì—­ì‹œ ë™ë¬¼ë³´í˜¸ì„¼í„°ì—ì„œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!\n\nâ–¶ê³µê³  í™•ì¸í•˜ê¸°:\nhttps://www.kcanimal.or.kr/board_gallery01/board_content.asp?board_idx={board_idx}&tname=board_gallery01"

                                        # 2-5. "ì‹ í˜¸" INSERT
                                        create_notification_signal(owner_user_num, message, noti_type="SCHEDULED")

                                        alerted_owners_for_board[board_idx].add(owner_user_num)
                                           
                    except Exception as e:
                        print(f"  [âŒ Trigger 1 ì˜¤ë¥˜] ì‹ ê·œ ë°ì´í„°(tuple: {new_animal_tuple[0]}) ë¹„êµ ì¤‘ ì‹¤íŒ¨: {e}")
            
    # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±°
    data_list = list(set(tuple(row) for row in all_data))
    
    print(f"[INFO] í¬ë¡¤ë§ëœ ìœ íš¨ ë°ì´í„° í•­ëª© ìˆ˜ (ì¤‘ë³µ ì œê±° í›„): {len(data_list)}ê°œ")
    
    if not data_list:
        print("âš ï¸ í¬ë¡¤ë§ëœ ìœ íš¨ ë°ì´í„°ê°€ ì—†ì–´ DB ì‘ì—…ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 3. ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° CSV ì €ì¥
    # ğŸ’¡ í¬ë¡¤ë§ëœ ë°ì´í„°(13ê°œ: BOARD_IDX + 11ê°œ í•­ëª© + CRAWL_URL)ì— íƒ€ì„ìŠ¤íƒ¬í”„(1ê°œ)ë¥¼ ì¶”ê°€í•˜ì—¬ 14ê°œ ì»¬ëŸ¼ì— ë§ì¶¤
    df = pd.DataFrame([row + (job_timestamp,) for row in data_list], columns=ANIMAL_COLUMNS) 
    timestamp_str = job_timestamp.strftime("%Y%m%d_%H%M%S")
    csv_filename = f"{DB_TABLE_NAME}_{timestamp_str}.csv"
    
    try:
        df.to_csv(csv_filename, header=True, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')
        print(f"âœ… Data saved successfully to {csv_filename}")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 4. MySQL ì—°ê²° ë° ì €ì¥ (UPSERT & DELETE)
    conn = None
    curs = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        curs = conn.cursor()
        
        # 4.1. DB UPSERT ì¿¼ë¦¬ ìƒì„±
        column_names = ANIMAL_COLUMNS
        value_placeholders = ', '.join(['%s'] * len(column_names))
        
        # LAST_CRAWLED_ATê³¼ CRAWL_URL, NAME, SPECIES ë“± ë¹„-ê³ ìœ  í‚¤ ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸
        update_cols = [
            f'`{c}` = VALUES(`{c}`)' 
            for c in column_names 
            if c not in UNIQUE_KEY_COLUMNS # BOARD_IDXë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
        ]
        update_set_clause = ', '.join(update_cols)
        
        sql_upsert = f"""
        INSERT INTO {DB_TABLE_NAME} ({', '.join(f'`{c}`' for c in column_names)}) 
        VALUES({value_placeholders})
        ON DUPLICATE KEY UPDATE
            {update_set_clause};
        """ 
        
        data_to_insert = [tuple(row) for row in df.values]
        
        rows_processed = curs.executemany(sql_upsert, data_to_insert)
        
        conn.commit()
        
        print(f"âœ… DB UPSERT ì™„ë£Œ. ì´ {rows_processed}ê°œ ë ˆì½”ë“œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤ (ì‚½ì…/ì—…ë°ì´íŠ¸ í¬í•¨).")
        
        # 4.2. ì‚¬ë¼ì§„ ë°ì´í„° ì‚­ì œ (DELETE)
        sql_delete_old = f"""
        DELETE FROM {DB_TABLE_NAME} 
        WHERE LAST_CRAWLED_AT < %s;
        """
        rows_deleted = curs.execute(sql_delete_old, (job_timestamp,))
        
        conn.commit()
        
        print(f"âœ… ì‚¬ë¼ì§„ ë°ì´í„° ì‚­ì œ ì™„ë£Œ. ì´ {rows_deleted}ê°œ ë ˆì½”ë“œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ DB ì‘ì—… ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if conn:
            conn.rollback()
            print("âŒ DB ë¡¤ë°± ì™„ë£Œ.")
            
    finally:
        if curs: curs.close()
        if conn: conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ.")
    
    print("\n-------------------------------------------------------")
    print("ğŸš€ [Step 2] AI ë°ì´í„°(JSON/Index) ìë™ ê°±ì‹ ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("-------------------------------------------------------")

    try:
        # 1. íŒŒì¼ ê°±ì‹  (llm_animal.pyì— ìƒˆë¡œ ë§Œë“  í•¨ìˆ˜ í˜¸ì¶œ)
        # ì£¼ì˜: llm_animal.pyì— 'refresh_crawled_data' í•¨ìˆ˜ê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        if hasattr(llm_animal, 'refresh_crawled_data'):
            success = llm_animal.refresh_crawled_data()
        else:
            print("âŒ [ì˜¤ë¥˜] llm_animal.pyì— 'refresh_crawled_data' í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            success = False

        if success:
            # 2. ì‹¤í–‰ ì¤‘ì¸ Flask ì„œë²„ì—ê²Œ "ë©”ëª¨ë¦¬ ìƒˆë¡œê³ ì¹¨(Hot Reload)" ìš”ì²­
            # (WAS ì„œë²„ê°€ ë¡œì»¬í˜¸ìŠ¤íŠ¸ 5000ë²ˆì— ìˆë‹¤ê³  ê°€ì •)
            print("ğŸ“¡ [Step 3] Flask ì„œë²„ ë©”ëª¨ë¦¬ ìƒˆë¡œê³ ì¹¨ ìš”ì²­ ì¤‘...")
            try:
                # íƒ€ì„ì•„ì›ƒì„ ë„‰ë„‰í•˜ê²Œ 10ë¶„(600ì´ˆ)ìœ¼ë¡œ ì„¤ì •
                response = requests.post("http://localhost:5000/api/refresh_index", timeout=600)

                if response.status_code == 200:
                    print(f"âœ… ì„œë²„ ë©”ëª¨ë¦¬ ê°±ì‹  ì„±ê³µ: {response.json().get('message')}")
                else:
                    print(f"âš ï¸ ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
            except Exception as req_err:
                print(f"âš ï¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ì„œë²„ê°€ êº¼ì ¸ìˆì„ ìˆ˜ ìˆìŒ): {req_err}")
        else:
            print("âŒ AI ë°ì´í„° íŒŒì¼ ê°±ì‹ ì— ì‹¤íŒ¨í•˜ì—¬ ì„œë²„ ìš”ì²­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ìë™ ê°±ì‹  í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ====================================================================
# 6. ìŠ¤ì¼€ì¤„ ì„¤ì • ë° ì‹¤í–‰ ë£¨í”„ 
# ====================================================================

if __name__ == '__main__':
    # 1. DB ìŠ¤í‚¤ë§ˆ(UNIQUE KEY) ì´ˆê¸°í™”
    initialize_db_schema()
    
    # 2. (ìˆ˜ì •) â—€ ìŠ¤ì¼€ì¤„ë§ ì—†ì´, job_crawl_and_save í•¨ìˆ˜ë¥¼ 1íšŒë§Œ ì‹¤í–‰
    print("=======================================================")
    print(f"[MAIN] ë™ë¬¼ ë°ì´í„° í¬ë¡¤ë§ ì‘ì—…ì„ 1íšŒ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    print("=======================================================") 

    # ìµœì´ˆ 1íšŒ ì‹¤í–‰
    job_crawl_and_save()

    print("=======================================================")
    print(f"[MAIN] ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    print("=======================================================")
